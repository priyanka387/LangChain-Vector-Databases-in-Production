Hugging Face Models Datasets Spaces Docs Solutions Pricing Log In Sign Up Hub Python Library documentation How-to guides Hub Python Library Search documentation mainv0.19.3v0.18.0.rc0v0.17.3v0.16.3v0.15.1v0.14.1v0.13.4v0.12.1v0.11.0v0.10.1v0.9.1v0.8.1v0.7.0.rc0v0.6.0.rc0v0.5.1 DEENHIKO Get started Home Quickstart Installation How-to guides Overview Download files Upload files Use the CLI HfFileSystem Repository Search Inference Inference Endpoints Community Tab Collections Cache Model Cards Manage your Space Integrate a library Webhooks server Conceptual guides Git vs HTTP paradigm Reference Overview Login and logout Environment variables Managing local and online repositories Hugging Face Hub API Downloading files Mixins & serialization methods Inference Client Inference Endpoints HfFileSystem Utilities Discussions and Pull Requests Cache-system reference Repo Cards and Repo Card Data Space runtime Collections TensorBoard logger Webhooks server Join the Hugging Face community and get access to the augmented documentation experience Collaborate on models, datasets and Spaces Faster examples with accelerated inference Switch between documentation themes Sign Up to get started How-to guides In this section, you will find practical guides to help you achieve a specific goal. Take a look at these guides to learn how to use huggingface_hub to solve real-world problems: Repository How to create a repository on the Hub? How to configure it? How to interact with it? Download files How do I download a file from the Hub? How do I download a repository? Upload files How to upload a file or a folder? How to make changes to an existing repository on the Hub? Search How to efficiently search through the 200k+ public models, datasets and spaces? HfFileSystem How to interact with the Hub through a convenient interface that mimics Python's file interface? Inference How to make predictions using the accelerated Inference API? Community Tab How to interact with the Community tab (Discussions and Pull Requests)? Collections How to programmatically build collections? Cache How does the cache-system work? How to benefit from it? Model Cards How to create and share Model Cards? Manage your Space How to manage your Space hardware and configuration? Integrate a library What does it mean to integrate a library with the Hub? And how to do it? Webhooks server How to create a server to receive Webhooks and deploy it as a Space? â†Installation Download filesâ†’ How-to guides
Hugging Face Models Datasets Spaces Docs Solutions Pricing Log In Sign Up Hub Python Library documentation Download files from the Hub Hub Python Library Search documentation mainv0.19.3v0.18.0.rc0v0.17.3v0.16.3v0.15.1v0.14.1v0.13.4v0.12.1v0.11.0v0.10.1v0.9.1v0.8.1v0.7.0.rc0v0.6.0.rc0v0.5.1 DEENHIKO Get started Home Quickstart Installation How-to guides Overview Download files Upload files Use the CLI HfFileSystem Repository Search Inference Inference Endpoints Community Tab Collections Cache Model Cards Manage your Space Integrate a library Webhooks server Conceptual guides Git vs HTTP paradigm Reference Overview Login and logout Environment variables Managing local and online repositories Hugging Face Hub API Downloading files Mixins & serialization methods Inference Client Inference Endpoints HfFileSystem Utilities Discussions and Pull Requests Cache-system reference Repo Cards and Repo Card Data Space runtime Collections TensorBoard logger Webhooks server Join the Hugging Face community and get access to the augmented documentation experience Collaborate on models, datasets and Spaces Faster examples with accelerated inference Switch between documentation themes Sign Up to get started Download files from the Hub The huggingface_hub library provides functions to download files from the repositories stored on the Hub. You can use these functions independently or integrate them into your own library, making it more convenient for your users to interact with the Hub. This guide will show you how to: Download and cache a single file. Download and cache an entire repository. Download files to a local folder. Download a single file The hf_hub_download() function is the main function for downloading files from the Hub. It downloads the remote file, caches it on disk (in a version-aware way), and returns its local file path. The returned filepath is a pointer to the HF local cache. Therefore, it is important to not modify the file to avoid having a corrupted cache. If you are interested in getting to know more about how files are cached, please refer to our caching guide. From latest version Select the file to download using the repo_id, repo_type and filename parameters. By default, the file will be considered as being part of a model repo. Copied >>> from huggingface_hub import hf_hub_download >>> hf_hub_download(repo_id="lysandre/arxiv-nlp", filename="config.json") '/root/.cache/huggingface/hub/models--lysandre--arxiv-nlp/snapshots/894a9adde21d9a3e3843e6d5aeaaf01875c7fade/config.json' # Download from a dataset >>> hf_hub_download(repo_id="google/fleurs", filename="fleurs.py", repo_type="dataset") '/root/.cache/huggingface/hub/datasets--google--fleurs/snapshots/199e4ae37915137c555b1765c01477c216287d34/fleurs.py' From specific version By default, the latest version from the main branch is downloaded. However, in some cases you want to download a file at a particular version (e.g. from a specific branch, a PR, a tag or a commit hash). To do so, use the revision parameter: Copied # Download from the `v1.0` tag >>> hf_hub_download(repo_id="lysandre/arxiv-nlp", filename="config.json", revision="v1.0") # Download from the `test-branch` branch >>> hf_hub_download(repo_id="lysandre/arxiv-nlp", filename="config.json", revision="test-branch") # Download from Pull Request #3 >>> hf_hub_download(repo_id="lysandre/arxiv-nlp", filename="config.json", revision="refs/pr/3") # Download from a specific commit hash >>> hf_hub_download(repo_id="lysandre/arxiv-nlp", filename="config.json", revision="877b84a8f93f2d619faa2a6e514a32beef88ab0a") Note: When using the commit hash, it must be the full-length hash instead of a 7-character commit hash. Construct a download URL In case you want to construct the URL used to download a file from a repo, you can use hf_hub_url() which returns a URL. Note that it is used internally by hf_hub_download(). Download an entire repository snapshot_download() downloads an entire repository at a given revision. It uses internally hf_hub_download() which means all downloaded files are also cached on your local disk. Downloads are made concurrently to speed-up the process. To download a whole repository, just pass the repo_id and repo_type: Copied >>> from huggingface_hub import snapshot_download >>> snapshot_download(repo_id="lysandre/arxiv-nlp") '/home/lysandre/.cache/huggingface/hub/models--lysandre--arxiv-nlp/snapshots/894a9adde21d9a3e3843e6d5aeaaf01875c7fade' # Or from a dataset >>> snapshot_download(repo_id="google/fleurs", repo_type="dataset") '/home/lysandre/.cache/huggingface/hub/datasets--google--fleurs/snapshots/199e4ae37915137c555b1765c01477c216287d34' snapshot_download() downloads the latest revision by default. If you want a specific repository revision, use the revision parameter: Copied >>> from huggingface_hub import snapshot_download >>> snapshot_download(repo_id="lysandre/arxiv-nlp", revision="refs/pr/1") Filter files to download snapshot_download() provides an easy way to download a repository. However, you donâ€™t always want to download the entire content of a repository. For example, you might want to prevent downloading all .bin files if you know youâ€™ll only use the .safetensors weights. You can do that using allow_patterns and ignore_patterns parameters. These parameters accept either a single pattern or a list of patterns. Patterns are Standard Wildcards (globbing patterns) as documented here. The pattern matching is based on fnmatch. For example, you can use allow_patterns to only download JSON configuration files: Copied >>> from huggingface_hub import snapshot_download >>> snapshot_download(repo_id="lysandre/arxiv-nlp", allow_patterns="*.json") On the other hand, ignore_patterns can exclude certain files from being downloaded. The following example ignores the .msgpack and .h5 file extensions: Copied >>> from huggingface_hub import snapshot_download >>> snapshot_download(repo_id="lysandre/arxiv-nlp", ignore_patterns=["*.msgpack", "*.h5"]) Finally, you can combine both to precisely filter your download. Here is an example to download all json and markdown files except vocab.json. Copied >>> from huggingface_hub import snapshot_download >>> snapshot_download(repo_id="gpt2", allow_patterns=["*.md", "*.json"], ignore_patterns="vocab.json") Download file(s) to local folder The recommended (and default) way to download files from the Hub is to use the cache-system. You can define your cache location by setting cache_dir parameter (both in hf_hub_download() and snapshot_download()). However, in some cases you want to download files and move them to a specific folder. This is useful to get a workflow closer to what git commands offer. You can do that using the local_dir and local_dir_use_symlinks parameters: local_dir must be a path to a folder on your system. The downloaded files will keep the same file structure as in the repo. For example if filename="data/train.csv" and local_dir="path/to/folder", then the returned filepath will be "path/to/folder/data/train.csv". local_dir_use_symlinks defines how the file must be saved in your local folder.The default behavior ("auto") is to duplicate small files (<5MB) and use symlinks for bigger files. Symlinks allow to optimize both bandwidth and disk usage. However manually editing a symlinked file might corrupt the cache, hence the duplication for small files. The 5MB threshold can be configured with the HF_HUB_LOCAL_DIR_AUTO_SYMLINK_THRESHOLD environment variable. If local_dir_use_symlinks=True is set, all files are symlinked for an optimal disk space optimization. This is for example useful when downloading a huge dataset with thousands of small files. Finally, if you donâ€™t want symlinks at all you can disable them (local_dir_use_symlinks=False). The cache directory will still be used to check wether the file is already cached or not. If already cached, the file is duplicated from the cache (i.e. saves bandwidth but increases disk usage). If the file is not already cached, it will be downloaded and moved directly to the local dir. This means that if you need to reuse it somewhere else later, it will be re-downloaded. Here is a table that summarizes the different options to help you choose the parameters that best suit your use case. Parameters File already cached Returned path Can read path? Can save to path? Optimized bandwidth Optimized disk usage local_dir=None symlink in cache âœ… âŒ(save would corrupt the cache) âœ… âœ… local_dir="path/to/folder"local_dir_use_symlinks="auto" file or symlink in folder âœ… âœ… (for small files) âš ï¸ (for big files do not resolve path before saving) âœ… âœ… local_dir="path/to/folder"local_dir_use_symlinks=True symlink in folder âœ… âš ï¸(do not resolve path before saving) âœ… âœ… local_dir="path/to/folder"local_dir_use_symlinks=False No file in folder âœ… âœ… âŒ(if re-run, file is re-downloaded) âš ï¸(multiple copies if ran in multiple folders) local_dir="path/to/folder"local_dir_use_symlinks=False Yes file in folder âœ… âœ… âš ï¸(file has to be cached first) âŒ(file is duplicated) Note: if you are on a Windows machine, you need to enable developer mode or run huggingface_hub as admin to enable symlinks. Check out the cache limitations section for more details. Download from the CLI You can use the huggingface-cli download command from the terminal to directly download files from the Hub. Internally, it uses the same hf_hub_download() and snapshot_download() helpers described above and prints the returned path to the terminal. Copied >>> huggingface-cli download gpt2 config.json /home/wauplin/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/config.json You can download multiple files at once which displays a progress bar and returns the snapshot path in which the files are located: Copied >>> huggingface-cli download gpt2 config.json model.safetensors Fetching 2 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 23831.27it/s] /home/wauplin/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10 For more details about the CLI download command, please refer to the CLI guide. Faster downloads If you are running on a machine with high bandwidth, you can increase your download speed with hf_transfer, a Rust-based library developed to speed up file transfers with the Hub. To enable it, install the package (pip install hf_transfer) and set HF_HUB_ENABLE_HF_TRANSFER=1 as an environment variable. Progress bars are supported in hf_transfer starting from version 0.1.4. Consider upgrading (pip install -U hf-transfer) if you plan to enable faster downloads. hf_transfer is a power user tool! It is tested and production-ready, but it lacks user-friendly features like advanced error handling or proxies. For more details, please take a look at this section. â†Overview Upload filesâ†’ Download files from the Hub Download a single file From latest version From specific version Construct a download URL Download an entire repository Filter files to download Download file(s) to local folder Download from the CLI Faster downloads
Hugging Face Models Datasets Spaces Docs Solutions Pricing Log In Sign Up Hub Python Library documentation Upload files to the Hub Hub Python Library Search documentation mainv0.19.3v0.18.0.rc0v0.17.3v0.16.3v0.15.1v0.14.1v0.13.4v0.12.1v0.11.0v0.10.1v0.9.1v0.8.1v0.7.0.rc0v0.6.0.rc0v0.5.1 DEENHIKO Get started Home Quickstart Installation How-to guides Overview Download files Upload files Use the CLI HfFileSystem Repository Search Inference Inference Endpoints Community Tab Collections Cache Model Cards Manage your Space Integrate a library Webhooks server Conceptual guides Git vs HTTP paradigm Reference Overview Login and logout Environment variables Managing local and online repositories Hugging Face Hub API Downloading files Mixins & serialization methods Inference Client Inference Endpoints HfFileSystem Utilities Discussions and Pull Requests Cache-system reference Repo Cards and Repo Card Data Space runtime Collections TensorBoard logger Webhooks server Join the Hugging Face community and get access to the augmented documentation experience Collaborate on models, datasets and Spaces Faster examples with accelerated inference Switch between documentation themes Sign Up to get started Upload files to the Hub Sharing your files and work is an important aspect of the Hub. The huggingface_hub offers several options for uploading your files to the Hub. You can use these functions independently or integrate them into your library, making it more convenient for your users to interact with the Hub. This guide will show you how to push files: without using Git. that are very large with Git LFS. with the commit context manager. with the push_to_hub() function. Whenever you want to upload files to the Hub, you need to log in to your Hugging Face account: Log in to your Hugging Face account with the following command: Copied huggingface-cli login # or using an environment variable huggingface-cli login --token $HUGGINGFACE_TOKEN Alternatively, you can programmatically login using login() in a notebook or a script: Copied >>> from huggingface_hub import login >>> login() If ran in a Jupyter or Colaboratory notebook, login() will launch a widget from which you can enter your Hugging Face access token. Otherwise, a message will be prompted in the terminal. It is also possible to login programmatically without the widget by directly passing the token to login(). If you do so, be careful when sharing your notebook. It is best practice to load the token from a secure vault instead of saving it in plain in your Colaboratory notebook. Upload a file Once youâ€™ve created a repository with create_repo(), you can upload a file to your repository using upload_file(). Specify the path of the file to upload, where you want to upload the file to in the repository, and the name of the repository you want to add the file to. Depending on your repository type, you can optionally set the repository type as a dataset, model, or space. Copied >>> from huggingface_hub import HfApi >>> api = HfApi() >>> api.upload_file( ... path_or_fileobj="/path/to/local/folder/README.md", ... path_in_repo="README.md", ... repo_id="username/test-dataset", ... repo_type="dataset", ... ) Upload a folder Use the upload_folder() function to upload a local folder to an existing repository. Specify the path of the local folder to upload, where you want to upload the folder to in the repository, and the name of the repository you want to add the folder to. Depending on your repository type, you can optionally set the repository type as a dataset, model, or space. Copied >>> from huggingface_hub import HfApi >>> api = HfApi() # Upload all the content from the local folder to your remote Space. # By default, files are uploaded at the root of the repo >>> api.upload_folder( ... folder_path="/path/to/local/space", ... repo_id="username/my-cool-space", ... repo_type="space", ... ) Use the allow_patterns and ignore_patterns arguments to specify which files to upload. These parameters accept either a single pattern or a list of patterns. Patterns are Standard Wildcards (globbing patterns) as documented here. If both allow_patterns and ignore_patterns are provided, both constraints apply. By default, all files from the folder are uploaded. Any .git/ folder present in any subdirectory will be ignored. However, please be aware that the .gitignore file is not taken into account. This means you must use allow_patterns and ignore_patterns to specify which files to upload instead. Copied >>> api.upload_folder( ... folder_path="/path/to/local/folder", ... path_in_repo="my-dataset/train", # Upload to a specific folder ... repo_id="username/test-dataset", ... repo_type="dataset", ... ignore_patterns="**/logs/*.txt", # Ignore all text logs ... ) You can also use the delete_patterns argument to specify files you want to delete from the repo in the same commit. This can prove useful if you want to clean a remote folder before pushing files in it and you donâ€™t know which files already exists. The example below uploads the local ./logs folder to the remote /experiment/logs/ folder. Only txt files are uploaded but before that, all previous logs on the repo on deleted. All of this in a single commit. Copied >>> api.upload_folder( ... folder_path="/path/to/local/folder/logs", ... repo_id="username/trained-model", ... path_in_repo="experiment/logs/", ... allow_patterns="*.txt", # Upload all local text files ... delete_patterns="*.txt", # Delete all remote text files before ... ) Upload from the CLI You can use the huggingface-cli upload command from the terminal to directly upload files to the Hub. Internally it uses the same upload_file() and upload_folder() helpers described above. You can either upload a single file or an entire folder: Copied # Usage: huggingface-cli upload [repo_id] [local_path] [path_in_repo] >>> huggingface-cli upload Wauplin/my-cool-model ./models/model.safetensors model.safetensors https://huggingface.co/Wauplin/my-cool-model/blob/main/model.safetensors >>> huggingface-cli upload Wauplin/my-cool-model ./models . https://huggingface.co/Wauplin/my-cool-model/tree/main local_path and path_in_repo are optional and can be implicitly inferred. If local_path is not set, the tool will check if a local folder or file has the same name as the repo_id. If thatâ€™s the case, its content will be uploaded. Otherwise, an exception is raised asking the user to explicitly set local_path. In any case, if path_in_repo is not set, files are uploaded at the root of the repo. For more details about the CLI upload command, please refer to the CLI guide. Advanced features In most cases, you wonâ€™t need more than upload_file() and upload_folder() to upload your files to the Hub. However, huggingface_hub has more advanced features to make things easier. Letâ€™s have a look at them! Non-blocking uploads In some cases, you want to push data without blocking your main thread. This is particularly useful to upload logs and artifacts while continuing a training. To do so, you can use the run_as_future argument in both upload_file() and upload_folder(). This will return a concurrent.futures.Future object that you can use to check the status of the upload. Copied >>> from huggingface_hub import HfApi >>> api = HfApi() >>> future = api.upload_folder( # Upload in the background (non-blocking action) ... repo_id="username/my-model", ... folder_path="checkpoints-001", ... run_as_future=True, ... ) >>> future Future(...) >>> future.done() False >>> future.result() # Wait for the upload to complete (blocking action) ... Background jobs are queued when using run_as_future=True. This means that you are guaranteed that the jobs will be executed in the correct order. Even though background jobs are mostly useful to upload data/create commits, you can queue any method you like using run_as_future(). For instance, you can use it to create a repo and then upload data to it in the background. The built-in run_as_future argument in upload methods is just an alias around it. Copied >>> from huggingface_hub import HfApi >>> api = HfApi() >>> api.run_as_future(api.create_repo, "username/my-model", exists_ok=True) Future(...) >>> api.upload_file( ... repo_id="username/my-model", ... path_in_repo="file.txt", ... path_or_fileobj=b"file content", ... run_as_future=True, ... ) Future(...) Upload a folder by chunks upload_folder() makes it easy to upload an entire folder to the Hub. However, for large folders (thousands of files or hundreds of GB), it can still be challenging. If you have a folder with a lot of files, you might want to upload it in several commits. If you experience an error or a connection issue during the upload, you would not have to resume the process from the beginning. To upload a folder in multiple commits, just pass multi_commits=True as argument. Under the hood, huggingface_hub will list the files to upload/delete and split them in several commits. The â€œstrategyâ€ (i.e. how to split the commits) is based on the number and size of the files to upload. A PR is open on the Hub to push all the commits. Once the PR is ready, the commits are squashed into a single commit. If the process is interrupted before completing, you can rerun your script to resume the upload. The created PR will be automatically detected and the upload will resume from where it stopped. It is recommended to pass multi_commits_verbose=True to get a better understanding of the upload and its progress. The example below will upload the checkpoints folder to a dataset in multiple commits. A PR will be created on the Hub and merged automatically once the upload is complete. If you prefer the PR to stay open and review it manually, you can pass create_pr=True. Copied >>> upload_folder( ... folder_path="local/checkpoints", ... repo_id="username/my-dataset", ... repo_type="dataset", ... multi_commits=True, ... multi_commits_verbose=True, ... ) If you want a better control on the upload strategy (i.e. the commits that are created), you can have a look at the low-level plan_multi_commits() and create_commits_on_pr() methods. multi_commits is still an experimental feature. Its API and behavior is subject to change in the future without prior notice. Scheduled uploads The Hugging Face Hub makes it easy to save and version data. However, there are some limitations when updating the same file thousands of times. For instance, you might want to save logs of a training process or user feedback on a deployed Space. In these cases, uploading the data as a dataset on the Hub makes sense, but it can be hard to do properly. The main reason is that you donâ€™t want to version every update of your data because itâ€™ll make the git repository unusable. The CommitScheduler class offers a solution to this problem. The idea is to run a background job that regularly pushes a local folder to the Hub. Letâ€™s assume you have a Gradio Space that takes as input some text and generates two translations of it. Then, the user can select their preferred translation. For each run, you want to save the input, output, and user preference to analyze the results. This is a perfect use case for CommitScheduler; you want to save data to the Hub (potentially millions of user feedback), but you donâ€™t need to save in real-time each userâ€™s input. Instead, you can save the data locally in a JSON file and upload it every 10 minutes. For example: Copied >>> import json >>> import uuid >>> from pathlib import Path >>> import gradio as gr >>> from huggingface_hub import CommitScheduler # Define the file where to save the data. Use UUID to make sure not to overwrite existing data from a previous run. >>> feedback_file = Path("user_feedback/") / f"data_{uuid.uuid4()}.json" >>> feedback_folder = feedback_file.parent # Schedule regular uploads. Remote repo and local folder are created if they don't already exist. >>> scheduler = CommitScheduler( ... repo_id="report-translation-feedback", ... repo_type="dataset", ... folder_path=feedback_folder, ... path_in_repo="data", ... every=10, ... ) # Define the function that will be called when the user submits its feedback (to be called in Gradio) >>> def save_feedback(input_text:str, output_1: str, output_2:str, user_choice: int) -> None: ... """ ... Append input/outputs and user feedback to a JSON Lines file using a thread lock to avoid concurrent writes from different users. ... """ ... with scheduler.lock: ... with feedback_file.open("a") as f: ... f.write(json.dumps({"input": input_text, "output_1": output_1, "output_2": output_2, "user_choice": user_choice})) ... f.write("\n") # Start Gradio >>> with gr.Blocks() as demo: >>> ... # define Gradio demo + use `save_feedback` >>> demo.launch() And thatâ€™s it! User input/outputs and feedback will be available as a dataset on the Hub. By using a unique JSON file name, you are guaranteed you wonâ€™t overwrite data from a previous run or data from another Spaces/replicas pushing concurrently to the same repository. For more details about the CommitScheduler, here is what you need to know: append-only: It is assumed that you will only add content to the folder. You must only append data to existing files or create new files. Deleting or overwriting a file might corrupt your repository. git history: The scheduler will commit the folder every every minutes. To avoid polluting the git repository too much, it is recommended to set a minimal value of 5 minutes. Besides, the scheduler is designed to avoid empty commits. If no new content is detected in the folder, the scheduled commit is dropped. errors: The scheduler run as background thread. It is started when you instantiate the class and never stops. In particular, if an error occurs during the upload (example: connection issue), the scheduler will silently ignore it and retry at the next scheduled commit. thread-safety: In most cases it is safe to assume that you can write to a file without having to worry about a lock file. The scheduler will not crash or be corrupted if you write content to the folder while itâ€™s uploading. In practice, it is possible that concurrency issues happen for heavy-loaded apps. In this case, we advice to use the scheduler.lock lock to ensure thread-safety. The lock is blocked only when the scheduler scans the folder for changes, not when it uploads data. You can safely assume that it will not affect the user experience on your Space. Space persistence demo Persisting data from a Space to a Dataset on the Hub is the main use case for CommitScheduler. Depending on the use case, you might want to structure your data differently. The structure has to be robust to concurrent users and restarts which often implies generating UUIDs. Besides robustness, you should upload data in a format readable by the ðŸ¤— Datasets library for later reuse. We created a Space that demonstrates how to save several different data formats (you may need to adapt it for your own specific needs). Custom uploads CommitScheduler assumes your data is append-only and should be uploading â€œas isâ€. However, you might want to customize the way data is uploaded. You can do that by creating a class inheriting from CommitScheduler and overwrite the push_to_hub method (feel free to overwrite it any way you want). You are guaranteed it will be called every every minutes in a background thread. You donâ€™t have to worry about concurrency and errors but you must be careful about other aspects, such as pushing empty commits or duplicated data. In the (simplified) example below, we overwrite push_to_hub to zip all PNG files in a single archive to avoid overloading the repo on the Hub: Copied class ZipScheduler(CommitScheduler): def push_to_hub(self): # 1. List PNG files png_files = list(self.folder_path.glob("*.png")) if len(png_files) == 0: return None # return early if nothing to commit # 2. Zip png files in a single archive with tempfile.TemporaryDirectory() as tmpdir: archive_path = Path(tmpdir) / "train.zip" with zipfile.ZipFile(archive_path, "w", zipfile.ZIP_DEFLATED) as zip: for png_file in png_files: zip.write(filename=png_file, arcname=png_file.name) # 3. Upload archive self.api.upload_file(..., path_or_fileobj=archive_path) # 4. Delete local png files to avoid re-uploading them later for png_file in png_files: png_file.unlink() When you overwrite push_to_hub, you have access to the attributes of CommitScheduler and especially: HfApi client: api Folder parameters: folder_path and path_in_repo Repo parameters: repo_id, repo_type, revision The thread lock: lock For more examples of custom schedulers, check out our demo Space containing different implementations depending on your use cases. create_commit The upload_file() and upload_folder() functions are high-level APIs that are generally convenient to use. We recommend trying these functions first if you donâ€™t need to work at a lower level. However, if you want to work at a commit-level, you can use the create_commit() function directly. There are three types of operations supported by create_commit(): CommitOperationAdd uploads a file to the Hub. If the file already exists, the file contents are overwritten. This operation accepts two arguments: path_in_repo: the repository path to upload a file to. path_or_fileobj: either a path to a file on your filesystem or a file-like object. This is the content of the file to upload to the Hub. CommitOperationDelete removes a file or a folder from a repository. This operation accepts path_in_repo as an argument. CommitOperationCopy copies a file within a repository. This operation accepts three arguments: src_path_in_repo: the repository path of the file to copy. path_in_repo: the repository path where the file should be copied. src_revision: optional - the revision of the file to copy if your want to copy a file from a different branch/revision. For example, if you want to upload two files and delete a file in a Hub repository: Use the appropriate CommitOperation to add or delete a file and to delete a folder: Copied >>> from huggingface_hub import HfApi, CommitOperationAdd, CommitOperationDelete >>> api = HfApi() >>> operations = [ ... CommitOperationAdd(path_in_repo="LICENSE.md", path_or_fileobj="~/repo/LICENSE.md"), ... CommitOperationAdd(path_in_repo="weights.h5", path_or_fileobj="~/repo/weights-final.h5"), ... CommitOperationDelete(path_in_repo="old-weights.h5"), ... CommitOperationDelete(path_in_repo="logs/"), ... CommitOperationCopy(src_path_in_repo="image.png", path_in_repo="duplicate_image.png"), ... ] Pass your operations to create_commit(): Copied >>> api.create_commit( ... repo_id="lysandre/test-model", ... operations=operations, ... commit_message="Upload my model weights and license", ... ) In addition to upload_file() and upload_folder(), the following functions also use create_commit() under the hood: delete_file() deletes a single file from a repository on the Hub. delete_folder() deletes an entire folder from a repository on the Hub. metadata_update() updates a repositoryâ€™s metadata. For more detailed information, take a look at the HfApi reference. Preupload LFS files before commit In some cases, you might want to upload huge files to S3 before making the commit call. For example, if you are committing a dataset in several shards that are generated in-memory, you would need to upload the shards one by one to avoid an out-of-memory issue. A solution is to upload each shard as a separate commit on the repo. While being perfectly valid, this solution has the drawback of potentially messing the git history by generating tens of commits. To overcome this issue, you can upload your files one by one to S3 and then create a single commit at the end. This is possible using preupload_lfs_files() in combination with create_commit(). This is a power-user method. Directly using upload_file(), upload_folder() or create_commit() instead of handling the low-level logic of pre-uploading files is the way to go in the vast majority of cases. The main caveat of preupload_lfs_files() is that until the commit is actually made, the upload files are not accessible on the repo on the Hub. If you have a question, feel free to ping us on our Discord or in a GitHub issue. Here is a simple example illustrating how to pre-upload files: Copied >>> from huggingface_hub import CommitOperationAdd, preupload_lfs_files, create_commit, create_repo >>> repo_id = create_repo("test_preupload").repo_id >>> operations = [] # List of all `CommitOperationAdd` objects that will be generated >>> for i in range(5): ... content = ... # generate binary content ... addition = CommitOperationAdd(path_in_repo=f"shard_{i}_of_5.bin", path_or_fileobj=content) ... preupload_lfs_files(repo_id, additions=[addition]) ... operations.append(addition) >>> # Create commit >>> create_commit(repo_id, operations=operations, commit_message="Commit all shards") First, we create the CommitOperationAdd objects one by one. In a real-world example, those would contain the generated shards. Each file is uploaded before generating the next one. During the preupload_lfs_files() step, the CommitOperationAdd object is mutated. You should only use it to pass it directly to create_commit(). The main update of the object is that the binary content is removed from it, meaning that it will be garbage-collected if you donâ€™t store another reference to it. This is expected as we donâ€™t want to keep in memory the content that is already uploaded. Finally we create the commit by passing all the operations to create_commit(). You can pass additional operations (add, delete or copy) that have not been processed yet and they will be handled correctly. Tips and tricks for large uploads There are some limitations to be aware of when dealing with a large amount of data in your repo. Given the time it takes to stream the data, getting an upload/push to fail at the end of the process or encountering a degraded experience, be it on hf.co or when working locally, can be very annoying. Check out our Repository limitations and recommendations guide for best practices on how to structure your repositories on the Hub. Next, letâ€™s move on with some practical tips to make your upload process as smooth as possible. Start small: We recommend starting with a small amount of data to test your upload script. Itâ€™s easier to iterate on a script when failing takes only a little time. Expect failures: Streaming large amounts of data is challenging. You donâ€™t know what can happen, but itâ€™s always best to consider that something will fail at least once -no matter if itâ€™s due to your machine, your connection, or our servers. For example, if you plan to upload a large number of files, itâ€™s best to keep track locally of which files you already uploaded before uploading the next batch. You are ensured that an LFS file that is already committed will never be re-uploaded twice but checking it client-side can still save some time. Use hf_transfer: this is a Rust-based library meant to speed up uploads on machines with very high bandwidth. To use it, you must install it (pip install hf_transfer) and enable it by setting HF_HUB_ENABLE_HF_TRANSFER=1 as an environment variable. You can then use huggingface_hub normally. Disclaimer: this is a power user tool. It is tested and production-ready but lacks user-friendly features like advanced error handling or proxies. For more details, please refer to this section. Progress bars are supported in hf_transfer starting from version 0.1.4. Consider upgrading (pip install -U hf-transfer) if you plan to enable faster uploads. (legacy) Upload files with Git LFS All the methods described above use the Hubâ€™s API to upload files. This is the recommended way to upload files to the Hub. However, we also provide Repository(), a wrapper around the git tool to manage a local repository. Although Repository() is not formally deprecated, we recommend using the HTTP-based methods described above instead. For more details about this recommendation, please have a look at this guide explaining the core differences between HTTP-based and Git-based approaches. Git LFS automatically handles files larger than 10MB. But for very large files (>5GB), you need to install a custom transfer agent for Git LFS: Copied huggingface-cli lfs-enable-largefiles You should install this for each repository that has a very large file. Once installed, youâ€™ll be able to push files larger than 5GB. commit context manager The commit context manager handles four of the most common Git commands: pull, add, commit, and push. git-lfs automatically tracks any file larger than 10MB. In the following example, the commit context manager: Pulls from the text-files repository. Adds a change made to file.txt. Commits the change. Pushes the change to the text-files repository. Copied >>> from huggingface_hub import Repository >>> with Repository(local_dir="text-files", clone_from="<user>/text-files").commit(commit_message="My first file :)"): ... with open("file.txt", "w+") as f: ... f.write(json.dumps({"hey": 8})) Here is another example of how to use the commit context manager to save and upload a file to a repository: Copied >>> import torch >>> model = torch.nn.Transformer() >>> with Repository("torch-model", clone_from="<user>/torch-model", token=True).commit(commit_message="My cool model :)"): ... torch.save(model.state_dict(), "model.pt") Set blocking=False if you would like to push your commits asynchronously. Non-blocking behavior is helpful when you want to continue running your script while your commits are being pushed. Copied >>> with repo.commit(commit_message="My cool model :)", blocking=False) You can check the status of your push with the command_queue method: Copied >>> last_command = repo.command_queue[-1] >>> last_command.status Refer to the table below for the possible statuses: Status Description -1 The push is ongoing. 0 The push has completed successfully. Non-zero An error has occurred. When blocking=False, commands are tracked, and your script will only exit when all pushes are completed, even if other errors occur in your script. Some additional useful commands for checking the status of a push include: Copied # Inspect an error. >>> last_command.stderr # Check whether a push is completed or ongoing. >>> last_command.is_done # Check whether a push command has errored. >>> last_command.failed push_to_hub The Repository() class has a push_to_hub() function to add files, make a commit, and push them to a repository. Unlike the commit context manager, youâ€™ll need to pull from a repository first before calling push_to_hub(). For example, if youâ€™ve already cloned a repository from the Hub, then you can initialize the repo from the local directory: Copied >>> from huggingface_hub import Repository >>> repo = Repository(local_dir="path/to/local/repo") Update your local clone with git_pull() and then push your file to the Hub: Copied >>> repo.git_pull() >>> repo.push_to_hub(commit_message="Commit my-awesome-file to the Hub") However, if you arenâ€™t ready to push a file yet, you can use git_add() and git_commit() to only add and commit your file: Copied >>> repo.git_add("path/to/file") >>> repo.git_commit(commit_message="add my first model config file :)") When youâ€™re ready, push the file to your repository with git_push(): Copied >>> repo.git_push() â†Download files Use the CLIâ†’ Upload files to the Hub Upload a file Upload a folder Upload from the CLI Advanced features Non-blocking uploads Upload a folder by chunks Scheduled uploads Space persistence demoCustom uploadscreate_commit Preupload LFS files before commit Tips and tricks for large uploads (legacy) Upload files with Git LFS commit context manager push_to_hub
Hugging Face Models Datasets Spaces Docs Solutions Pricing Log In Sign Up Hub Python Library documentation Interact with the Hub through the Filesystem API Hub Python Library Search documentation mainv0.19.3v0.18.0.rc0v0.17.3v0.16.3v0.15.1v0.14.1v0.13.4v0.12.1v0.11.0v0.10.1v0.9.1v0.8.1v0.7.0.rc0v0.6.0.rc0v0.5.1 DEENHIKO Get started Home Quickstart Installation How-to guides Overview Download files Upload files Use the CLI HfFileSystem Repository Search Inference Inference Endpoints Community Tab Collections Cache Model Cards Manage your Space Integrate a library Webhooks server Conceptual guides Git vs HTTP paradigm Reference Overview Login and logout Environment variables Managing local and online repositories Hugging Face Hub API Downloading files Mixins & serialization methods Inference Client Inference Endpoints HfFileSystem Utilities Discussions and Pull Requests Cache-system reference Repo Cards and Repo Card Data Space runtime Collections TensorBoard logger Webhooks server Join the Hugging Face community and get access to the augmented documentation experience Collaborate on models, datasets and Spaces Faster examples with accelerated inference Switch between documentation themes Sign Up to get started Interact with the Hub through the Filesystem API In addition to the HfApi, the huggingface_hub library provides HfFileSystem, a pythonic fsspec-compatible file interface to the Hugging Face Hub. The HfFileSystem builds of top of the HfApi and offers typical filesystem style operations like cp, mv, ls, du, glob, get_file, and put_file. Usage Copied >>> from huggingface_hub import HfFileSystem >>> fs = HfFileSystem() >>> # List all files in a directory >>> fs.ls("datasets/my-username/my-dataset-repo/data", detail=False) ['datasets/my-username/my-dataset-repo/data/train.csv', 'datasets/my-username/my-dataset-repo/data/test.csv'] >>> # List all ".csv" files in a repo >>> fs.glob("datasets/my-username/my-dataset-repo/**.csv") ['datasets/my-username/my-dataset-repo/data/train.csv', 'datasets/my-username/my-dataset-repo/data/test.csv'] >>> # Read a remote file >>> with fs.open("datasets/my-username/my-dataset-repo/data/train.csv", "r") as f: ... train_data = f.readlines() >>> # Read the content of a remote file as a string >>> train_data = fs.read_text("datasets/my-username/my-dataset-repo/data/train.csv", revision="dev") >>> # Write a remote file >>> with fs.open("datasets/my-username/my-dataset-repo/data/validation.csv", "w") as f: ... f.write("text,label") ... f.write("Fantastic movie!,good") The optional revision argument can be passed to run an operation from a specific commit such as a branch, tag name, or a commit hash. Unlike Pythonâ€™s built-in open, fsspecâ€™s open defaults to binary mode, "rb". This means you must explicitly set mode as "r" for reading and "w" for writing in text mode. Appending to a file (modes "a" and "ab") is not supported yet. Integrations The HfFileSystem can be used with any library that integrates fsspec, provided the URL follows the scheme: Copied hf://[<repo_type_prefix>]<repo_id>[@<revision>]/<path/in/repo> The repo_type_prefix is datasets/ for datasets, spaces/ for spaces, and models donâ€™t need a prefix in the URL. Some interesting integrations where HfFileSystem simplifies interacting with the Hub are listed below: Reading/writing a Pandas DataFrame from/to a Hub repository: Copied >>> import pandas as pd >>> # Read a remote CSV file into a dataframe >>> df = pd.read_csv("hf://datasets/my-username/my-dataset-repo/train.csv") >>> # Write a dataframe to a remote CSV file >>> df.to_csv("hf://datasets/my-username/my-dataset-repo/test.csv") The same workflow can also be used for Dask and Polars DataFrames. Querying (remote) Hub files with DuckDB: Copied >>> from huggingface_hub import HfFileSystem >>> import duckdb >>> fs = HfFileSystem() >>> duckdb.register_filesystem(fs) >>> # Query a remote file and get the result back as a dataframe >>> fs_query_file = "hf://datasets/my-username/my-dataset-repo/data_dir/data.parquet" >>> df = duckdb.query(f"SELECT * FROM '{fs_query_file}' LIMIT 10").df() Using the Hub as an array store with Zarr: Copied >>> import numpy as np >>> import zarr >>> embeddings = np.random.randn(50000, 1000).astype("float32") >>> # Write an array to a repo >>> with zarr.open_group("hf://my-username/my-model-repo/array-store", mode="w") as root: ... foo = root.create_group("embeddings") ... foobar = foo.zeros('experiment_0', shape=(50000, 1000), chunks=(10000, 1000), dtype='f4') ... foobar[:] = embeddings >>> # Read an array from a repo >>> with zarr.open_group("hf://my-username/my-model-repo/array-store", mode="r") as root: ... first_row = root["embeddings/experiment_0"][0] Authentication In many cases, you must be logged in with a Hugging Face account to interact with the Hub. Refer to the Login section of the documentation to learn more about authentication methods on the Hub. It is also possible to login programmatically by passing your token as an argument to HfFileSystem: Copied >>> from huggingface_hub import HfFileSystem >>> fs = HfFileSystem(token=token) If you login this way, be careful not to accidentally leak the token when sharing your source code! â†Use the CLI Repositoryâ†’ Interact with the Hub through the Filesystem API Usage Integrations Authentication
Hugging Face Models Datasets Spaces Docs Solutions Pricing Log In Sign Up Hub Python Library documentation Create and manage a repository Hub Python Library Search documentation mainv0.19.3v0.18.0.rc0v0.17.3v0.16.3v0.15.1v0.14.1v0.13.4v0.12.1v0.11.0v0.10.1v0.9.1v0.8.1v0.7.0.rc0v0.6.0.rc0v0.5.1 DEENHIKO Get started Home Quickstart Installation How-to guides Overview Download files Upload files Use the CLI HfFileSystem Repository Search Inference Inference Endpoints Community Tab Collections Cache Model Cards Manage your Space Integrate a library Webhooks server Conceptual guides Git vs HTTP paradigm Reference Overview Login and logout Environment variables Managing local and online repositories Hugging Face Hub API Downloading files Mixins & serialization methods Inference Client Inference Endpoints HfFileSystem Utilities Discussions and Pull Requests Cache-system reference Repo Cards and Repo Card Data Space runtime Collections TensorBoard logger Webhooks server Join the Hugging Face community and get access to the augmented documentation experience Collaborate on models, datasets and Spaces Faster examples with accelerated inference Switch between documentation themes Sign Up to get started Create and manage a repository The Hugging Face Hub is a collection of git repositories. Git is a widely used tool in software development to easily version projects when working collaboratively. This guide will show you how to interact with the repositories on the Hub, especially: Create and delete a repository. Manage branches and tags. Rename your repository. Update your repository visibility. Manage a local copy of your repository. If you are used to working with platforms such as GitLab/GitHub/Bitbucket, your first instinct might be to use git CLI to clone your repo (git clone), commit changes (git add, git commit) and push them (git push). This is valid when using the Hugging Face Hub. However, software engineering and machine learning do not share the same requirements and workflows. Model repositories might maintain large model weight files for different frameworks and tools, so cloning the repository can lead to you maintaining large local folders with massive sizes. As a result, it may be more efficient to use our custom HTTP methods. You can read our Git vs HTTP paradigm explanation page for more details. If you want to create and manage a repository on the Hub, your machine must be logged in. If you are not, please refer to this section. In the rest of this guide, we will assume that your machine is logged in. Repo creation and deletion The first step is to know how to create and delete repositories. You can only manage repositories that you own (under your username namespace) or from organizations in which you have write permissions. Create a repository Create an empty repository with create_repo() and give it a name with the repo_id parameter. The repo_id is your namespace followed by the repository name: username_or_org/repo_name. Copied >>> from huggingface_hub import create_repo >>> create_repo("lysandre/test-model") 'https://huggingface.co/lysandre/test-model' By default, create_repo() creates a model repository. But you can use the repo_type parameter to specify another repository type. For example, if you want to create a dataset repository: Copied >>> from huggingface_hub import create_repo >>> create_repo("lysandre/test-dataset", repo_type="dataset") 'https://huggingface.co/datasets/lysandre/test-dataset' When you create a repository, you can set your repository visibility with the private parameter. Copied >>> from huggingface_hub import create_repo >>> create_repo("lysandre/test-private", private=True) If you want to change the repository visibility at a later time, you can use the update_repo_visibility() function. Delete a repository Delete a repository with delete_repo(). Make sure you want to delete a repository because this is an irreversible process! Specify the repo_id of the repository you want to delete: Copied >>> delete_repo(repo_id="lysandre/my-corrupted-dataset", repo_type="dataset") Duplicate a repository (only for Spaces) In some cases, you want to copy someone elseâ€™s repo to adapt it to your use case. This is possible for Spaces using the duplicate_space() method. It will duplicate the whole repository. You will still need to configure your own settings (hardware, sleep-time, storage, variables and secrets). Check out our Manage your Space guide for more details. Copied >>> from huggingface_hub import duplicate_space >>> duplicate_space("multimodalart/dreambooth-training", private=False) RepoUrl('https://huggingface.co/spaces/nateraw/dreambooth-training',...) Upload and download files Now that you have created your repository, you are interested in pushing changes to it and downloading files from it. These 2 topics deserve their own guides. Please refer to the upload and the download guides to learn how to use your repository. Branches and tags Git repositories often make use of branches to store different versions of a same repository. Tags can also be used to flag a specific state of your repository, for example, when releasing a version. More generally, branches and tags are referred as git references. Create branches and tags You can create new branch and tags using create_branch() and create_tag(): Copied >>> from huggingface_hub import create_branch, create_tag # Create a branch on a Space repo from `main` branch >>> create_branch("Matthijs/speecht5-tts-demo", repo_type="space", branch="handle-dog-speaker") # Create a tag on a Dataset repo from `v0.1-release` branch >>> create_branch("bigcode/the-stack", repo_type="dataset", revision="v0.1-release", tag="v0.1.1", tag_message="Bump release version.") You can use the delete_branch() and delete_tag() functions in the same way to delete a branch or a tag. List all branches and tags You can also list the existing git refs from a repository using list_repo_refs(): Copied >>> from huggingface_hub import list_repo_refs >>> list_repo_refs("bigcode/the-stack", repo_type="dataset") GitRefs( branches=[ GitRefInfo(name='main', ref='refs/heads/main', target_commit='18edc1591d9ce72aa82f56c4431b3c969b210ae3'), GitRefInfo(name='v1.1.a1', ref='refs/heads/v1.1.a1', target_commit='f9826b862d1567f3822d3d25649b0d6d22ace714') ], converts=[], tags=[ GitRefInfo(name='v1.0', ref='refs/tags/v1.0', target_commit='c37a8cd1e382064d8aced5e05543c5f7753834da') ] ) Change repository settings Repositories come with some settings that you can configure. Most of the time, you will want to do that manually in the repo settings page in your browser. You must have write access to a repo to configure it (either own it or being part of an organization). In this section, we will see the settings that you can also configure programmatically using huggingface_hub. Some settings are specific to Spaces (hardware, environment variables,â€¦). To configure those, please refer to our Manage your Spaces guide. Update visibility A repository can be public or private. A private repository is only visible to you or members of the organization in which the repository is located. Change a repository to private as shown in the following: Copied >>> from huggingface_hub import update_repo_visibility >>> update_repo_visibility(repo_id=repo_id, private=True) Rename your repository You can rename your repository on the Hub using move_repo(). Using this method, you can also move the repo from a user to an organization. When doing so, there are a few limitations that you should be aware of. For example, you canâ€™t transfer your repo to another user. Copied >>> from huggingface_hub import move_repo >>> move_repo(from_id="Wauplin/cool-model", to_id="huggingface/cool-model") Manage a local copy of your repository All the actions described above can be done using HTTP requests. However, in some cases you might be interested in having a local copy of your repository and interact with it using the Git commands you are familiar with. The Repository() class allows you to interact with files and repositories on the Hub with functions similar to Git commands. It is a wrapper over Git and Git-LFS methods to use the Git commands you already know and love. Before starting, please make sure you have Git-LFS installed (see here for installation instructions). Repository() is deprecated in favor of the http-based alternatives implemented in HfApi. Given its large adoption in legacy code, the complete removal of Repository() will only happen in release v1.0. For more details, please read this explanation page. Use a local repository Instantiate a Repository() object with a path to a local repository: Copied >>> from huggingface_hub import Repository >>> repo = Repository(local_dir="<path>/<to>/<folder>") Clone The clone_from parameter clones a repository from a Hugging Face repository ID to a local directory specified by the local_dir argument: Copied >>> from huggingface_hub import Repository >>> repo = Repository(local_dir="w2v2", clone_from="facebook/wav2vec2-large-960h-lv60") clone_from can also clone a repository using a URL: Copied >>> repo = Repository(local_dir="huggingface-hub", clone_from="https://huggingface.co/facebook/wav2vec2-large-960h-lv60") You can combine the clone_from parameter with create_repo() to create and clone a repository: Copied >>> repo_url = create_repo(repo_id="repo_name") >>> repo = Repository(local_dir="repo_local_path", clone_from=repo_url) You can also configure a Git username and email to a cloned repository by specifying the git_user and git_email parameters when you clone a repository. When users commit to that repository, Git will be aware of the commit author. Copied >>> repo = Repository( ... "my-dataset", ... clone_from="<user>/<dataset_id>", ... token=True, ... repo_type="dataset", ... git_user="MyName", ... git_email="me@cool.mail" ... ) Branch Branches are important for collaboration and experimentation without impacting your current files and code. Switch between branches with git_checkout(). For example, if you want to switch from branch1 to branch2: Copied >>> from huggingface_hub import Repository >>> repo = Repository(local_dir="huggingface-hub", clone_from="<user>/<dataset_id>", revision='branch1') >>> repo.git_checkout("branch2") Pull git_pull() allows you to update a current local branch with changes from a remote repository: Copied >>> from huggingface_hub import Repository >>> repo.git_pull() Set rebase=True if you want your local commits to occur after your branch is updated with the new commits from the remote: Copied >>> repo.git_pull(rebase=True) â†HfFileSystem Searchâ†’ Create and manage a repository Repo creation and deletion Create a repository Delete a repository Duplicate a repository (only for Spaces) Upload and download files Branches and tags Create branches and tags List all branches and tags Change repository settings Update visibility Rename your repository Manage a local copy of your repository Use a local repository Clone Branch Pull
Hugging Face Models Datasets Spaces Docs Solutions Pricing Log In Sign Up Hub Python Library documentation Search the Hub Hub Python Library Search documentation mainv0.19.3v0.18.0.rc0v0.17.3v0.16.3v0.15.1v0.14.1v0.13.4v0.12.1v0.11.0v0.10.1v0.9.1v0.8.1v0.7.0.rc0v0.6.0.rc0v0.5.1 DEENHIKO Get started Home Quickstart Installation How-to guides Overview Download files Upload files Use the CLI HfFileSystem Repository Search Inference Inference Endpoints Community Tab Collections Cache Model Cards Manage your Space Integrate a library Webhooks server Conceptual guides Git vs HTTP paradigm Reference Overview Login and logout Environment variables Managing local and online repositories Hugging Face Hub API Downloading files Mixins & serialization methods Inference Client Inference Endpoints HfFileSystem Utilities Discussions and Pull Requests Cache-system reference Repo Cards and Repo Card Data Space runtime Collections TensorBoard logger Webhooks server Join the Hugging Face community and get access to the augmented documentation experience Collaborate on models, datasets and Spaces Faster examples with accelerated inference Switch between documentation themes Sign Up to get started Search the Hub In this tutorial, you will learn how to search models, datasets and spaces on the Hub using huggingface_hub. How to list repositories ? huggingface_hub library includes an HTTP client HfApi to interact with the Hub. Among other things, it can list models, datasets and spaces stored on the Hub: Copied >>> from huggingface_hub import HfApi >>> api = HfApi() >>> models = api.list_models() The output of list_models() is an iterator over the models stored on the Hub. Similarly, you can use list_datasets() to list datasets and list_spaces() to list Spaces. How to filter repositories ? Listing repositories is great but now you might want to filter your search. The list helpers have several attributes like: filter author search â€¦ Two of these parameters are intuitive (author and search), but what about that filter? filter takes as input a ModelFilter object (or DatasetFilter). You can instantiate it by specifying which models you want to filter. Letâ€™s see an example to get all models on the Hub that does image classification, have been trained on the imagenet dataset and that runs with PyTorch. That can be done with a single ModelFilter. Attributes are combined as â€œlogical ANDâ€. Copied models = hf_api.list_models( filter=ModelFilter( task="image-classification", library="pytorch", trained_dataset="imagenet" ) ) While filtering, you can also sort the models and take only the top results. For example, the following example fetches the top 5 most downloaded datasets on the Hub: Copied >>> list(list_datasets(sort="downloads", direction=-1, limit=5)) [DatasetInfo( id='argilla/databricks-dolly-15k-curated-en', author='argilla', sha='4dcd1dedbe148307a833c931b21ca456a1fc4281', last_modified=datetime.datetime(2023, 10, 2, 12, 32, 53, tzinfo=datetime.timezone.utc), private=False, downloads=8889377, (...) To explore available filter on the Hub, visit models and datasets pages in your browser, search for some parameters and look at the values in the URL. â†Repository Inferenceâ†’ Search the Hub How to list repositories ? How to filter repositories ?
Hugging Face Models Datasets Spaces Docs Solutions Pricing Log In Sign Up Hub Python Library documentation Run Inference on servers Hub Python Library Search documentation mainv0.19.3v0.18.0.rc0v0.17.3v0.16.3v0.15.1v0.14.1v0.13.4v0.12.1v0.11.0v0.10.1v0.9.1v0.8.1v0.7.0.rc0v0.6.0.rc0v0.5.1 DEENHIKO Get started Home Quickstart Installation How-to guides Overview Download files Upload files Use the CLI HfFileSystem Repository Search Inference Inference Endpoints Community Tab Collections Cache Model Cards Manage your Space Integrate a library Webhooks server Conceptual guides Git vs HTTP paradigm Reference Overview Login and logout Environment variables Managing local and online repositories Hugging Face Hub API Downloading files Mixins & serialization methods Inference Client Inference Endpoints HfFileSystem Utilities Discussions and Pull Requests Cache-system reference Repo Cards and Repo Card Data Space runtime Collections TensorBoard logger Webhooks server Join the Hugging Face community and get access to the augmented documentation experience Collaborate on models, datasets and Spaces Faster examples with accelerated inference Switch between documentation themes Sign Up to get started Run Inference on servers Inference is the process of using a trained model to make predictions on new data. As this process can be compute-intensive, running on a dedicated server can be an interesting option. The huggingface_hub library provides an easy way to call a service that runs inference for hosted models. There are several services you can connect to: Inference API: a service that allows you to run accelerated inference on Hugging Faceâ€™s infrastructure for free. This service is a fast way to get started, test different models, and prototype AI products. Inference Endpoints: a product to easily deploy models to production. Inference is run by Hugging Face in a dedicated, fully managed infrastructure on a cloud provider of your choice. These services can be called with the InferenceClient object. It acts as a replacement for the legacy InferenceApi client, adding specific support for tasks and handling inference on both Inference API and Inference Endpoints. Learn how to migrate to the new client in the Legacy InferenceAPI client section. InferenceClient is a Python client making HTTP calls to our APIs. If you want to make the HTTP calls directly using your preferred tool (curl, postman,â€¦), please refer to the Inference API or to the Inference Endpoints documentation pages. For web development, a JS client has been released. If you are interested in game development, you might have a look at our C# project. Getting started Letâ€™s get started with a text-to-image task: Copied >>> from huggingface_hub import InferenceClient >>> client = InferenceClient() >>> image = client.text_to_image("An astronaut riding a horse on the moon.") >>> image.save("astronaut.png") We initialized an InferenceClient with the default parameters. The only thing you need to know is the task you want to perform. By default, the client will connect to the Inference API and select a model to complete the task. In our example, we generated an image from a text prompt. The returned value is a PIL.Image object that can be saved to a file. The API is designed to be simple. Not all parameters and options are available or described for the end user. Check out this page if you are interested in learning more about all the parameters available for each task. Using a specific model What if you want to use a specific model? You can specify it either as a parameter or directly at an instance level: Copied >>> from huggingface_hub import InferenceClient # Initialize client for a specific model >>> client = InferenceClient(model="prompthero/openjourney-v4") >>> client.text_to_image(...) # Or use a generic client but pass your model as an argument >>> client = InferenceClient() >>> client.text_to_image(..., model="prompthero/openjourney-v4") There are more than 200k models on the Hugging Face Hub! Each task in the InferenceClient comes with a recommended model. Be aware that the HF recommendation can change over time without prior notice. Therefore it is best to explicitly set a model once you are decided. Also, in most cases youâ€™ll be interested in finding a model specific to your needs. Visit the Models page on the Hub to explore your possibilities. Using a specific URL The examples we saw above use the free-hosted Inference API. This proves to be very useful for prototyping and testing things quickly. Once youâ€™re ready to deploy your model to production, youâ€™ll need to use a dedicated infrastructure. Thatâ€™s where Inference Endpoints comes into play. It allows you to deploy any model and expose it as a private API. Once deployed, youâ€™ll get a URL that you can connect to using exactly the same code as before, changing only the model parameter: Copied >>> from huggingface_hub import InferenceClient >>> client = InferenceClient(model="https://uu149rez6gw9ehej.eu-west-1.aws.endpoints.huggingface.cloud/deepfloyd-if") # or >>> client = InferenceClient() >>> client.text_to_image(..., model="https://uu149rez6gw9ehej.eu-west-1.aws.endpoints.huggingface.cloud/deepfloyd-if") Authentication Calls made with the InferenceClient can be authenticated using a User Access Token. By default, it will use the token saved on your machine if you are logged in (check out how to login). If you are not logged in, you can pass your token as an instance parameter: Copied >>> from huggingface_hub import InferenceClient >>> client = InferenceClient(token="hf_***") Authentication is NOT mandatory when using the Inference API. However, authenticated users get a higher free-tier to play with the service. Token is also mandatory if you want to run inference on your private models or on private endpoints. Supported tasks InferenceClientâ€™s goal is to provide the easiest interface to run inference on Hugging Face models. It has a simple API that supports the most common tasks. Here is a list of the currently supported tasks: Domain Task Supported Documentation Audio Audio Classification âœ… audio_classification() Automatic Speech Recognition âœ… automatic_speech_recognition() Text-to-Speech âœ… text_to_speech() Computer Vision Image Classification âœ… image_classification() Image Segmentation âœ… image_segmentation() Image-to-Image âœ… image_to_image() Image-to-Text âœ… image_to_text() Object Detection âœ… object_detection() Text-to-Image âœ… text_to_image() Zero-Shot-Image-Classification âœ… zero_shot_image_classification() Multimodal Documentation Question Answering âœ… document_question_answering() Visual Question Answering âœ… visual_question_answering() NLP Conversational âœ… conversational() Feature Extraction âœ… feature_extraction() Fill Mask âœ… fill_mask() Question Answering âœ… question_answering() Sentence Similarity âœ… sentence_similarity() Summarization âœ… summarization() Table Question Answering âœ… table_question_answering() Text Classification âœ… text_classification() Text Generation âœ… text_generation() Token Classification âœ… token_classification() Translation âœ… translation() Zero Shot Classification âœ… zero_shot_classification() Tabular Tabular Classification âœ… tabular_classification() Tabular Regression âœ… tabular_regression() Check out the Tasks page to learn more about each task, how to use them, and the most popular models for each task. Custom requests However, it is not always possible to cover all use cases. For custom requests, the InferenceClient.post() method gives you the flexibility to send any request to the Inference API. For example, you can specify how to parse the inputs and outputs. In the example below, the generated image is returned as raw bytes instead of parsing it as a PIL Image. This can be helpful if you donâ€™t have Pillow installed in your setup and just care about the binary content of the image. InferenceClient.post() is also useful to handle tasks that are not yet officially supported. Copied >>> from huggingface_hub import InferenceClient >>> client = InferenceClient() >>> response = client.post(json={"inputs": "An astronaut riding a horse on the moon."}, model="stabilityai/stable-diffusion-2-1") >>> response.content # raw bytes b'...' Async client An async version of the client is also provided, based on asyncio and aiohttp. You can either install aiohttp directly or use the [inference] extra: Copied pip install --upgrade huggingface_hub[inference] # or # pip install aiohttp After installation all async API endpoints are available via AsyncInferenceClient. Its initialization and APIs are strictly the same as the sync-only version. Copied # Code must be run in a asyncio concurrent context. # $ python -m asyncio >>> from huggingface_hub import AsyncInferenceClient >>> client = AsyncInferenceClient() >>> image = await client.text_to_image("An astronaut riding a horse on the moon.") >>> image.save("astronaut.png") >>> async for token in await client.text_generation("The Huggingface Hub is", stream=True): ... print(token, end="") a platform for sharing and discussing ML-related content. For more information about the asyncio module, please refer to the official documentation. Advanced tips In the above section, we saw the main aspects of InferenceClient. Letâ€™s dive into some more advanced tips. Timeout When doing inference, there are two main causes for a timeout: The inference process takes a long time to complete. The model is not available, for example when Inference API is loading it for the first time. InferenceClient has a global timeout parameter to handle those two aspects. By default, it is set to None, meaning that the client will wait indefinitely for the inference to complete. If you want more control in your workflow, you can set it to a specific value in seconds. If the timeout delay expires, an InferenceTimeoutError is raised. You can catch it and handle it in your code: Copied >>> from huggingface_hub import InferenceClient, InferenceTimeoutError >>> client = InferenceClient(timeout=30) >>> try: ... client.text_to_image(...) ... except InferenceTimeoutError: ... print("Inference timed out after 30s.") Binary inputs Some tasks require binary inputs, for example, when dealing with images or audio files. In this case, InferenceClient tries to be as permissive as possible and accept different types: raw bytes a file-like object, opened as binary (with open("audio.flac", "rb") as f: ...) a path (str or Path) pointing to a local file a URL (str) pointing to a remote file (e.g. https://...). In this case, the file will be downloaded locally before sending it to the Inference API. Copied >>> from huggingface_hub import InferenceClient >>> client = InferenceClient() >>> client.image_classification("https://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Cute_dog.jpg/320px-Cute_dog.jpg") [{'score': 0.9779096841812134, 'label': 'Blenheim spaniel'}, ...] Legacy InferenceAPI client InferenceClient acts as a replacement for the legacy InferenceApi client. It adds specific support for tasks and handles inference on both Inference API and Inference Endpoints. Here is a short guide to help you migrate from InferenceApi to InferenceClient. Initialization Change from Copied >>> from huggingface_hub import InferenceApi >>> inference = InferenceApi(repo_id="bert-base-uncased", token=API_TOKEN) to Copied >>> from huggingface_hub import InferenceClient >>> inference = InferenceClient(model="bert-base-uncased", token=API_TOKEN) Run on a specific task Change from Copied >>> from huggingface_hub import InferenceApi >>> inference = InferenceApi(repo_id="paraphrase-xlm-r-multilingual-v1", task="feature-extraction") >>> inference(...) to Copied >>> from huggingface_hub import InferenceClient >>> inference = InferenceClient() >>> inference.feature_extraction(..., model="paraphrase-xlm-r-multilingual-v1") This is the recommended way to adapt your code to InferenceClient. It lets you benefit from the task-specific methods like feature_extraction. Run custom request Change from Copied >>> from huggingface_hub import InferenceApi >>> inference = InferenceApi(repo_id="bert-base-uncased") >>> inference(inputs="The goal of life is [MASK].") [{'sequence': 'the goal of life is life.', 'score': 0.10933292657136917, 'token': 2166, 'token_str': 'life'}] to Copied >>> from huggingface_hub import InferenceClient >>> client = InferenceClient() >>> response = client.post(json={"inputs": "The goal of life is [MASK]."}, model="bert-base-uncased") >>> response.json() [{'sequence': 'the goal of life is life.', 'score': 0.10933292657136917, 'token': 2166, 'token_str': 'life'}] Run with parameters Change from Copied >>> from huggingface_hub import InferenceApi >>> inference = InferenceApi(repo_id="typeform/distilbert-base-uncased-mnli") >>> inputs = "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!" >>> params = {"candidate_labels":["refund", "legal", "faq"]} >>> inference(inputs, params) {'sequence': 'Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!', 'labels': ['refund', 'faq', 'legal'], 'scores': [0.9378499388694763, 0.04914155602455139, 0.013008488342165947]} to Copied >>> from huggingface_hub import InferenceClient >>> client = InferenceClient() >>> inputs = "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!" >>> params = {"candidate_labels":["refund", "legal", "faq"]} >>> response = client.post(json={"inputs": inputs, "parameters": params}, model="typeform/distilbert-base-uncased-mnli") >>> response.json() {'sequence': 'Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!', 'labels': ['refund', 'faq', 'legal'], 'scores': [0.9378499388694763, 0.04914155602455139, 0.013008488342165947]} â†Search Inference Endpointsâ†’ Run Inference on servers Getting started Using a specific model Using a specific URL Authentication Supported tasks Custom requests Async client Advanced tips Timeout Binary inputs Legacy InferenceAPI client Initialization Run on a specific task Run custom request Run with parameters
Hugging Face Models Datasets Spaces Docs Solutions Pricing Log In Sign Up Hub Python Library documentation Interact with Discussions and Pull Requests Hub Python Library Search documentation mainv0.19.3v0.18.0.rc0v0.17.3v0.16.3v0.15.1v0.14.1v0.13.4v0.12.1v0.11.0v0.10.1v0.9.1v0.8.1v0.7.0.rc0v0.6.0.rc0v0.5.1 DEENHIKO Get started Home Quickstart Installation How-to guides Overview Download files Upload files Use the CLI HfFileSystem Repository Search Inference Inference Endpoints Community Tab Collections Cache Model Cards Manage your Space Integrate a library Webhooks server Conceptual guides Git vs HTTP paradigm Reference Overview Login and logout Environment variables Managing local and online repositories Hugging Face Hub API Downloading files Mixins & serialization methods Inference Client Inference Endpoints HfFileSystem Utilities Discussions and Pull Requests Cache-system reference Repo Cards and Repo Card Data Space runtime Collections TensorBoard logger Webhooks server Join the Hugging Face community and get access to the augmented documentation experience Collaborate on models, datasets and Spaces Faster examples with accelerated inference Switch between documentation themes Sign Up to get started Interact with Discussions and Pull Requests The huggingface_hub library provides a Python interface to interact with Pull Requests and Discussions on the Hub. Visit the dedicated documentation page for a deeper view of what Discussions and Pull Requests on the Hub are, and how they work under the hood. Retrieve Discussions and Pull Requests from the Hub The HfApi class allows you to retrieve Discussions and Pull Requests on a given repo: Copied >>> from huggingface_hub import get_repo_discussions >>> for discussion in get_repo_discussions(repo_id="bigscience/bloom-1b3"): ... print(f"{discussion.num} - {discussion.title}, pr: {discussion.is_pull_request}") # 11 - Add Flax weights, pr: True # 10 - Update README.md, pr: True # 9 - Training languages in the model card, pr: True # 8 - Update tokenizer_config.json, pr: True # 7 - Slurm training script, pr: False [...] HfApi.get_repo_discussions returns a generator that yields Discussion objects. To get all the Discussions in a single list, run: Copied >>> from huggingface_hub import get_repo_discussions >>> discussions_list = list(get_repo_discussions(repo_id="bert-base-uncased")) The Discussion object returned by HfApi.get_repo_discussions() contains high-level overview of the Discussion or Pull Request. You can also get more detailed information using HfApi.get_discussion_details(): Copied >>> from huggingface_hub import get_discussion_details >>> get_discussion_details( ... repo_id="bigscience/bloom-1b3", ... discussion_num=2 ... ) DiscussionWithDetails( num=2, author='cakiki', title='Update VRAM memory for the V100s', status='open', is_pull_request=True, events=[ DiscussionComment(type='comment', author='cakiki', ...), DiscussionCommit(type='commit', author='cakiki', summary='Update VRAM memory for the V100s', oid='1256f9d9a33fa8887e1c1bf0e09b4713da96773a', ...), ], conflicting_files=[], target_branch='refs/heads/main', merge_commit_oid=None, diff='diff --git a/README.md b/README.md\nindex a6ae3b9294edf8d0eda0d67c7780a10241242a7e..3a1814f212bc3f0d3cc8f74bdbd316de4ae7b9e3 100644\n--- a/README.md\n+++ b/README.md\n@@ -132,7 +132,7 [...]', ) HfApi.get_discussion_details() returns a DiscussionWithDetails object, which is a subclass of Discussion with more detailed information about the Discussion or Pull Request. Information includes all the comments, status changes, and renames of the Discussion via DiscussionWithDetails.events. In case of a Pull Request, you can retrieve the raw git diff with DiscussionWithDetails.diff. All the commits of the Pull Request are listed in DiscussionWithDetails.events. Create and edit a Discussion or Pull Request programmatically The HfApi class also offers ways to create and edit Discussions and Pull Requests. You will need an access token to create and edit Discussions or Pull Requests. The simplest way to propose changes on a repo on the Hub is via the create_commit() API: just set the create_pr parameter to True. This parameter is also available on other methods that wrap create_commit(): upload_file() upload_folder() delete_file() delete_folder() metadata_update() Copied >>> from huggingface_hub import metadata_update >>> metadata_update( ... repo_id="username/repo_name", ... metadata={"tags": ["computer-vision", "awesome-model"]}, ... create_pr=True, ... ) You can also use HfApi.create_discussion() (respectively HfApi.create_pull_request()) to create a Discussion (respectively a Pull Request) on a repo. Opening a Pull Request this way can be useful if you need to work on changes locally. Pull Requests opened this way will be in "draft" mode. Copied >>> from huggingface_hub import create_discussion, create_pull_request >>> create_discussion( ... repo_id="username/repo-name", ... title="Hi from the huggingface_hub library!", ... token="<insert your access token here>", ... ) DiscussionWithDetails(...) >>> create_pull_request( ... repo_id="username/repo-name", ... title="Hi from the huggingface_hub library!", ... token="<insert your access token here>", ... ) DiscussionWithDetails(..., is_pull_request=True) Managing Pull Requests and Discussions can be done entirely with the HfApi class. For example: comment_discussion() to add comments edit_discussion_comment() to edit comments rename_discussion() to rename a Discussion or Pull Request change_discussion_status() to open or close a Discussion / Pull Request merge_pull_request() to merge a Pull Request Visit the HfApi documentation page for an exhaustive reference of all available methods. Push changes to a Pull Request Coming soon ! See also For a more detailed reference, visit the Discussions and Pull Requests and the hf_api documentation page. â†Inference Endpoints Collectionsâ†’ Interact with Discussions and Pull Requests Retrieve Discussions and Pull Requests from the Hub Create and edit a Discussion or Pull Request programmatically Push changes to a Pull Request See also
Hugging Face Models Datasets Spaces Docs Solutions Pricing Log In Sign Up Hub Python Library documentation Manage huggingface_hub cache-system Hub Python Library Search documentation mainv0.19.3v0.18.0.rc0v0.17.3v0.16.3v0.15.1v0.14.1v0.13.4v0.12.1v0.11.0v0.10.1v0.9.1v0.8.1v0.7.0.rc0v0.6.0.rc0v0.5.1 DEENHIKO Get started Home Quickstart Installation How-to guides Overview Download files Upload files Use the CLI HfFileSystem Repository Search Inference Inference Endpoints Community Tab Collections Cache Model Cards Manage your Space Integrate a library Webhooks server Conceptual guides Git vs HTTP paradigm Reference Overview Login and logout Environment variables Managing local and online repositories Hugging Face Hub API Downloading files Mixins & serialization methods Inference Client Inference Endpoints HfFileSystem Utilities Discussions and Pull Requests Cache-system reference Repo Cards and Repo Card Data Space runtime Collections TensorBoard logger Webhooks server Join the Hugging Face community and get access to the augmented documentation experience Collaborate on models, datasets and Spaces Faster examples with accelerated inference Switch between documentation themes Sign Up to get started Manage huggingface_hub cache-system Understand caching The Hugging Face Hub cache-system is designed to be the central cache shared across libraries that depend on the Hub. It has been updated in v0.8.0 to prevent re-downloading same files between revisions. The caching system is designed as follows: Copied <CACHE_DIR> â”œâ”€ <MODELS> â”œâ”€ <DATASETS> â”œâ”€ <SPACES> The <CACHE_DIR> is usually your userâ€™s home directory. However, it is customizable with the cache_dir argument on all methods, or by specifying either HF_HOME or HF_HUB_CACHE environment variable. Models, datasets and spaces share a common root. Each of these repositories contains the repository type, the namespace (organization or username) if it exists and the repository name: Copied <CACHE_DIR> â”œâ”€ models--julien-c--EsperBERTo-small â”œâ”€ models--lysandrejik--arxiv-nlp â”œâ”€ models--bert-base-cased â”œâ”€ datasets--glue â”œâ”€ datasets--huggingface--DataMeasurementsFiles â”œâ”€ spaces--dalle-mini--dalle-mini It is within these folders that all files will now be downloaded from the Hub. Caching ensures that a file isnâ€™t downloaded twice if it already exists and wasnâ€™t updated; but if it was updated, and youâ€™re asking for the latest file, then it will download the latest file (while keeping the previous file intact in case you need it again). In order to achieve this, all folders contain the same skeleton: Copied <CACHE_DIR> â”œâ”€ datasets--glue â”‚ â”œâ”€ refs â”‚ â”œâ”€ blobs â”‚ â”œâ”€ snapshots ... Each folder is designed to contain the following: Refs The refs folder contains files which indicates the latest revision of the given reference. For example, if we have previously fetched a file from the main branch of a repository, the refs folder will contain a file named main, which will itself contain the commit identifier of the current head. If the latest commit of main has aaaaaa as identifier, then it will contain aaaaaa. If that same branch gets updated with a new commit, that has bbbbbb as an identifier, then re-downloading a file from that reference will update the refs/main file to contain bbbbbb. Blobs The blobs folder contains the actual files that we have downloaded. The name of each file is their hash. Snapshots The snapshots folder contains symlinks to the blobs mentioned above. It is itself made up of several folders: one per known revision! In the explanation above, we had initially fetched a file from the aaaaaa revision, before fetching a file from the bbbbbb revision. In this situation, we would now have two folders in the snapshots folder: aaaaaa and bbbbbb. In each of these folders, live symlinks that have the names of the files that we have downloaded. For example, if we had downloaded the README.md file at revision aaaaaa, we would have the following path: Copied <CACHE_DIR>/<REPO_NAME>/snapshots/aaaaaa/README.md That README.md file is actually a symlink linking to the blob that has the hash of the file. By creating the skeleton this way we open the mechanism to file sharing: if the same file was fetched in revision bbbbbb, it would have the same hash and the file would not need to be re-downloaded. .no_exist (advanced) In addition to the blobs, refs and snapshots folders, you might also find a .no_exist folder in your cache. This folder keeps track of files that youâ€™ve tried to download once but donâ€™t exist on the Hub. Its structure is the same as the snapshots folder with 1 subfolder per known revision: Copied <CACHE_DIR>/<REPO_NAME>/.no_exist/aaaaaa/config_that_does_not_exist.json Unlike the snapshots folder, files are simple empty files (no symlinks). In this example, the file "config_that_does_not_exist.json" does not exist on the Hub for the revision "aaaaaa". As it only stores empty files, this folder is neglectable is term of disk usage. So now you might wonder, why is this information even relevant? In some cases, a framework tries to load optional files for a model. Saving the non-existence of optional files makes it faster to load a model as it saves 1 HTTP call per possible optional file. This is for example the case in transformers where each tokenizer can support additional files. The first time you load the tokenizer on your machine, it will cache which optional files exists (and which doesnâ€™t) to make the loading time faster for the next initializations. To test if a file is cached locally (without making any HTTP request), you can use the try_to_load_from_cache() helper. It will either return the filepath (if exists and cached), the object _CACHED_NO_EXIST (if non-existence is cached) or None (if we donâ€™t know). Copied from huggingface_hub import try_to_load_from_cache, _CACHED_NO_EXIST filepath = try_to_load_from_cache() if isinstance(filepath, str): # file exists and is cached ... elif filepath is _CACHED_NO_EXIST: # non-existence of file is cached ... else: # file is not cached ... In practice In practice, your cache should look like the following tree: Copied [ 96] . â””â”€â”€ [ 160] models--julien-c--EsperBERTo-small â”œâ”€â”€ [ 160] blobs â”‚ â”œâ”€â”€ [321M] 403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd â”‚ â”œâ”€â”€ [ 398] 7cb18dc9bafbfcf74629a4b760af1b160957a83e â”‚ â””â”€â”€ [1.4K] d7edf6bd2a681fb0175f7735299831ee1b22b812 â”œâ”€â”€ [ 96] refs â”‚ â””â”€â”€ [ 40] main â””â”€â”€ [ 128] snapshots â”œâ”€â”€ [ 128] 2439f60ef33a0d46d85da5001d52aeda5b00ce9f â”‚ â”œâ”€â”€ [ 52] README.md -> ../../blobs/d7edf6bd2a681fb0175f7735299831ee1b22b812 â”‚ â””â”€â”€ [ 76] pytorch_model.bin -> ../../blobs/403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd â””â”€â”€ [ 128] bbc77c8132af1cc5cf678da3f1ddf2de43606d48 â”œâ”€â”€ [ 52] README.md -> ../../blobs/7cb18dc9bafbfcf74629a4b760af1b160957a83e â””â”€â”€ [ 76] pytorch_model.bin -> ../../blobs/403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd Limitations In order to have an efficient cache-system, huggingface-hub uses symlinks. However, symlinks are not supported on all machines. This is a known limitation especially on Windows. When this is the case, huggingface_hub do not use the blobs/ directory but directly stores the files in the snapshots/ directory instead. This workaround allows users to download and cache files from the Hub exactly the same way. Tools to inspect and delete the cache (see below) are also supported. However, the cache-system is less efficient as a single file might be downloaded several times if multiple revisions of the same repo is downloaded. If you want to benefit from the symlink-based cache-system on a Windows machine, you either need to activate Developer Mode or to run Python as an administrator. When symlinks are not supported, a warning message is displayed to the user to alert them they are using a degraded version of the cache-system. This warning can be disabled by setting the HF_HUB_DISABLE_SYMLINKS_WARNING environment variable to true. Caching assets In addition to caching files from the Hub, downstream libraries often requires to cache other files related to HF but not handled directly by huggingface_hub (example: file downloaded from GitHub, preprocessed data, logs,â€¦). In order to cache those files, called assets, one can use cached_assets_path(). This small helper generates paths in the HF cache in a unified way based on the name of the library requesting it and optionally on a namespace and a subfolder name. The goal is to let every downstream libraries manage its assets its own way (e.g. no rule on the structure) as long as it stays in the right assets folder. Those libraries can then leverage tools from huggingface_hub to manage the cache, in particular scanning and deleting parts of the assets from a CLI command. Copied from huggingface_hub import cached_assets_path assets_path = cached_assets_path(library_name="datasets", namespace="SQuAD", subfolder="download") something_path = assets_path / "something.json" # Do anything you like in your assets folder ! cached_assets_path() is the recommended way to store assets but is not mandatory. If your library already uses its own cache, feel free to use it! Assets in practice In practice, your assets cache should look like the following tree: Copied assets/ â””â”€â”€ datasets/ â”‚ â”œâ”€â”€ SQuAD/ â”‚ â”‚ â”œâ”€â”€ downloaded/ â”‚ â”‚ â”œâ”€â”€ extracted/ â”‚ â”‚ â””â”€â”€ processed/ â”‚ â”œâ”€â”€ Helsinki-NLP--tatoeba_mt/ â”‚ â”œâ”€â”€ downloaded/ â”‚ â”œâ”€â”€ extracted/ â”‚ â””â”€â”€ processed/ â””â”€â”€ transformers/ â”œâ”€â”€ default/ â”‚ â”œâ”€â”€ something/ â”œâ”€â”€ bert-base-cased/ â”‚ â”œâ”€â”€ default/ â”‚ â””â”€â”€ training/ hub/ â””â”€â”€ models--julien-c--EsperBERTo-small/ â”œâ”€â”€ blobs/ â”‚ â”œâ”€â”€ (...) â”‚ â”œâ”€â”€ (...) â”œâ”€â”€ refs/ â”‚ â””â”€â”€ (...) â””â”€â”€ [ 128] snapshots/ â”œâ”€â”€ 2439f60ef33a0d46d85da5001d52aeda5b00ce9f/ â”‚ â”œâ”€â”€ (...) â””â”€â”€ bbc77c8132af1cc5cf678da3f1ddf2de43606d48/ â””â”€â”€ (...) Scan your cache At the moment, cached files are never deleted from your local directory: when you download a new revision of a branch, previous files are kept in case you need them again. Therefore it can be useful to scan your cache directory in order to know which repos and revisions are taking the most disk space. huggingface_hub provides an helper to do so that can be used via huggingface-cli or in a python script. Scan cache from the terminal The easiest way to scan your HF cache-system is to use the scan-cache command from huggingface-cli tool. This command scans the cache and prints a report with information like repo id, repo type, disk usage, refs and full local path. The snippet below shows a scan report in a folder in which 4 models and 2 datasets are cached. Copied âžœ huggingface-cli scan-cache REPO ID REPO TYPE SIZE ON DISK NB FILES LAST_ACCESSED LAST_MODIFIED REFS LOCAL PATH --------------------------- --------- ------------ -------- ------------- ------------- ------------------- ------------------------------------------------------------------------- glue dataset 116.3K 15 4 days ago 4 days ago 2.4.0, main, 1.17.0 /home/wauplin/.cache/huggingface/hub/datasets--glue google/fleurs dataset 64.9M 6 1 week ago 1 week ago refs/pr/1, main /home/wauplin/.cache/huggingface/hub/datasets--google--fleurs Jean-Baptiste/camembert-ner model 441.0M 7 2 weeks ago 16 hours ago main /home/wauplin/.cache/huggingface/hub/models--Jean-Baptiste--camembert-ner bert-base-cased model 1.9G 13 1 week ago 2 years ago /home/wauplin/.cache/huggingface/hub/models--bert-base-cased t5-base model 10.1K 3 3 months ago 3 months ago main /home/wauplin/.cache/huggingface/hub/models--t5-base t5-small model 970.7M 11 3 days ago 3 days ago refs/pr/1, main /home/wauplin/.cache/huggingface/hub/models--t5-small Done in 0.0s. Scanned 6 repo(s) for a total of 3.4G. Got 1 warning(s) while scanning. Use -vvv to print details. To get a more detailed report, use the --verbose option. For each repo, you get a list of all revisions that have been downloaded. As explained above, the files that donâ€™t change between 2 revisions are shared thanks to the symlinks. This means that the size of the repo on disk is expected to be less than the sum of the size of each of its revisions. For example, here bert-base-cased has 2 revisions of 1.4G and 1.5G but the total disk usage is only 1.9G. Copied âžœ huggingface-cli scan-cache -v REPO ID REPO TYPE REVISION SIZE ON DISK NB FILES LAST_MODIFIED REFS LOCAL PATH --------------------------- --------- ---------------------------------------- ------------ -------- ------------- ----------- ---------------------------------------------------------------------------------------------------------------------------- glue dataset 9338f7b671827df886678df2bdd7cc7b4f36dffd 97.7K 14 4 days ago main, 2.4.0 /home/wauplin/.cache/huggingface/hub/datasets--glue/snapshots/9338f7b671827df886678df2bdd7cc7b4f36dffd glue dataset f021ae41c879fcabcf823648ec685e3fead91fe7 97.8K 14 1 week ago 1.17.0 /home/wauplin/.cache/huggingface/hub/datasets--glue/snapshots/f021ae41c879fcabcf823648ec685e3fead91fe7 google/fleurs dataset 129b6e96cf1967cd5d2b9b6aec75ce6cce7c89e8 25.4K 3 2 weeks ago refs/pr/1 /home/wauplin/.cache/huggingface/hub/datasets--google--fleurs/snapshots/129b6e96cf1967cd5d2b9b6aec75ce6cce7c89e8 google/fleurs dataset 24f85a01eb955224ca3946e70050869c56446805 64.9M 4 1 week ago main /home/wauplin/.cache/huggingface/hub/datasets--google--fleurs/snapshots/24f85a01eb955224ca3946e70050869c56446805 Jean-Baptiste/camembert-ner model dbec8489a1c44ecad9da8a9185115bccabd799fe 441.0M 7 16 hours ago main /home/wauplin/.cache/huggingface/hub/models--Jean-Baptiste--camembert-ner/snapshots/dbec8489a1c44ecad9da8a9185115bccabd799fe bert-base-cased model 378aa1bda6387fd00e824948ebe3488630ad8565 1.5G 9 2 years ago /home/wauplin/.cache/huggingface/hub/models--bert-base-cased/snapshots/378aa1bda6387fd00e824948ebe3488630ad8565 bert-base-cased model a8d257ba9925ef39f3036bfc338acf5283c512d9 1.4G 9 3 days ago main /home/wauplin/.cache/huggingface/hub/models--bert-base-cased/snapshots/a8d257ba9925ef39f3036bfc338acf5283c512d9 t5-base model 23aa4f41cb7c08d4b05c8f327b22bfa0eb8c7ad9 10.1K 3 1 week ago main /home/wauplin/.cache/huggingface/hub/models--t5-base/snapshots/23aa4f41cb7c08d4b05c8f327b22bfa0eb8c7ad9 t5-small model 98ffebbb27340ec1b1abd7c45da12c253ee1882a 726.2M 6 1 week ago refs/pr/1 /home/wauplin/.cache/huggingface/hub/models--t5-small/snapshots/98ffebbb27340ec1b1abd7c45da12c253ee1882a t5-small model d0a119eedb3718e34c648e594394474cf95e0617 485.8M 6 4 weeks ago /home/wauplin/.cache/huggingface/hub/models--t5-small/snapshots/d0a119eedb3718e34c648e594394474cf95e0617 t5-small model d78aea13fa7ecd06c29e3e46195d6341255065d5 970.7M 9 1 week ago main /home/wauplin/.cache/huggingface/hub/models--t5-small/snapshots/d78aea13fa7ecd06c29e3e46195d6341255065d5 Done in 0.0s. Scanned 6 repo(s) for a total of 3.4G. Got 1 warning(s) while scanning. Use -vvv to print details. Grep example Since the output is in tabular format, you can combine it with any grep-like tools to filter the entries. Here is an example to filter only revisions from the â€œt5-smallâ€ model on a Unix-based machine. Copied âžœ eval "huggingface-cli scan-cache -v" | grep "t5-small" t5-small model 98ffebbb27340ec1b1abd7c45da12c253ee1882a 726.2M 6 1 week ago refs/pr/1 /home/wauplin/.cache/huggingface/hub/models--t5-small/snapshots/98ffebbb27340ec1b1abd7c45da12c253ee1882a t5-small model d0a119eedb3718e34c648e594394474cf95e0617 485.8M 6 4 weeks ago /home/wauplin/.cache/huggingface/hub/models--t5-small/snapshots/d0a119eedb3718e34c648e594394474cf95e0617 t5-small model d78aea13fa7ecd06c29e3e46195d6341255065d5 970.7M 9 1 week ago main /home/wauplin/.cache/huggingface/hub/models--t5-small/snapshots/d78aea13fa7ecd06c29e3e46195d6341255065d5 Scan cache from Python For a more advanced usage, use scan_cache_dir() which is the python utility called by the CLI tool. You can use it to get a detailed report structured around 4 dataclasses: HFCacheInfo: complete report returned by scan_cache_dir() CachedRepoInfo: information about a cached repo CachedRevisionInfo: information about a cached revision (e.g. â€œsnapshotâ€) inside a repo CachedFileInfo: information about a cached file in a snapshot Here is a simple usage example. See reference for details. Copied >>> from huggingface_hub import scan_cache_dir >>> hf_cache_info = scan_cache_dir() HFCacheInfo( size_on_disk=3398085269, repos=frozenset({ CachedRepoInfo( repo_id='t5-small', repo_type='model', repo_path=PosixPath(...), size_on_disk=970726914, nb_files=11, last_accessed=1662971707.3567169, last_modified=1662971107.3567169, revisions=frozenset({ CachedRevisionInfo( commit_hash='d78aea13fa7ecd06c29e3e46195d6341255065d5', size_on_disk=970726339, snapshot_path=PosixPath(...), # No `last_accessed` as blobs are shared among revisions last_modified=1662971107.3567169, files=frozenset({ CachedFileInfo( file_name='config.json', size_on_disk=1197 file_path=PosixPath(...), blob_path=PosixPath(...), blob_last_accessed=1662971707.3567169, blob_last_modified=1662971107.3567169, ), CachedFileInfo(...), ... }), ), CachedRevisionInfo(...), ... }), ), CachedRepoInfo(...), ... }), warnings=[ CorruptedCacheException("Snapshots dir doesn't exist in cached repo: ..."), CorruptedCacheException(...), ... ], ) Clean your cache Scanning your cache is interesting but what you really want to do next is usually to delete some portions to free up some space on your drive. This is possible using the delete-cache CLI command. One can also programmatically use the delete_revisions() helper from HFCacheInfo object returned when scanning the cache. Delete strategy To delete some cache, you need to pass a list of revisions to delete. The tool will define a strategy to free up the space based on this list. It returns a DeleteCacheStrategy object that describes which files and folders will be deleted. The DeleteCacheStrategy allows give you how much space is expected to be freed. Once you agree with the deletion, you must execute it to make the deletion effective. In order to avoid discrepancies, you cannot edit a strategy object manually. The strategy to delete revisions is the following: the snapshot folder containing the revision symlinks is deleted. blobs files that are targeted only by revisions to be deleted are deleted as well. if a revision is linked to 1 or more refs, references are deleted. if all revisions from a repo are deleted, the entire cached repository is deleted. Revision hashes are unique across all repositories. This means you donâ€™t need to provide any repo_id or repo_type when removing revisions. If a revision is not found in the cache, it will be silently ignored. Besides, if a file or folder cannot be found while trying to delete it, a warning will be logged but no error is thrown. The deletion continues for other paths contained in the DeleteCacheStrategy object. Clean cache from the terminal The easiest way to delete some revisions from your HF cache-system is to use the delete-cache command from huggingface-cli tool. The command has two modes. By default, a TUI (Terminal User Interface) is displayed to the user to select which revisions to delete. This TUI is currently in beta as it has not been tested on all platforms. If the TUI doesnâ€™t work on your machine, you can disable it using the --disable-tui flag. Using the TUI This is the default mode. To use it, you first need to install extra dependencies by running the following command: Copied pip install huggingface_hub["cli"] Then run the command: Copied huggingface-cli delete-cache You should now see a list of revisions that you can select/deselect: Instructions: Press keyboard arrow keys <up> and <down> to move the cursor. Press <space> to toggle (select/unselect) an item. When a revision is selected, the first line is updated to show you how much space will be freed. Press <enter> to confirm your selection. If you want to cancel the operation and quit, you can select the first item (â€œNone of the followingâ€). If this item is selected, the delete process will be cancelled, no matter what other items are selected. Otherwise you can also press <ctrl+c> to quit the TUI. Once youâ€™ve selected the revisions you want to delete and pressed <enter>, a last confirmation message will be prompted. Press <enter> again and the deletion will be effective. If you want to cancel, enter n. Copied âœ— huggingface-cli delete-cache --dir ~/.cache/huggingface/hub ? Select revisions to delete: 2 revision(s) selected. ? 2 revisions selected counting for 3.1G. Confirm deletion ? Yes Start deletion. Done. Deleted 1 repo(s) and 0 revision(s) for a total of 3.1G. Without TUI As mentioned above, the TUI mode is currently in beta and is optional. It may be the case that it doesnâ€™t work on your machine or that you donâ€™t find it convenient. Another approach is to use the --disable-tui flag. The process is very similar as you will be asked to manually review the list of revisions to delete. However, this manual step will not take place in the terminal directly but in a temporary file generated on the fly and that you can manually edit. This file has all the instructions you need in the header. Open it in your favorite text editor. To select/deselect a revision, simply comment/uncomment it with a #. Once the manual review is done and the file is edited, you can save it. Go back to your terminal and press <enter>. By default it will compute how much space would be freed with the updated list of revisions. You can continue to edit the file or confirm with "y". Copied huggingface-cli delete-cache --disable-tui Example of command file: Copied # INSTRUCTIONS # ------------ # This is a temporary file created by running `huggingface-cli delete-cache` with the # `--disable-tui` option. It contains a set of revisions that can be deleted from your # local cache directory. # # Please manually review the revisions you want to delete: # - Revision hashes can be commented out with '#'. # - Only non-commented revisions in this file will be deleted. # - Revision hashes that are removed from this file are ignored as well. # - If `CANCEL_DELETION` line is uncommented, the all cache deletion is cancelled and # no changes will be applied. # # Once you've manually reviewed this file, please confirm deletion in the terminal. This # file will be automatically removed once done. # ------------ # KILL SWITCH # ------------ # Un-comment following line to completely cancel the deletion process # CANCEL_DELETION # ------------ # REVISIONS # ------------ # Dataset chrisjay/crowd-speech-africa (761.7M, used 5 days ago) ebedcd8c55c90d39fd27126d29d8484566cd27ca # Refs: main # modified 5 days ago # Dataset oscar (3.3M, used 4 days ago) # 916f956518279c5e60c63902ebdf3ddf9fa9d629 # Refs: main # modified 4 days ago # Dataset wikiann (804.1K, used 2 weeks ago) 89d089624b6323d69dcd9e5eb2def0551887a73a # Refs: main # modified 2 weeks ago # Dataset z-uo/male-LJSpeech-italian (5.5G, used 5 days ago) # 9cfa5647b32c0a30d0adfca06bf198d82192a0d1 # Refs: main # modified 5 days ago Clean cache from Python For more flexibility, you can also use the delete_revisions() method programmatically. Here is a simple example. See reference for details. Copied >>> from huggingface_hub import scan_cache_dir >>> delete_strategy = scan_cache_dir().delete_revisions( ... "81fd1d6e7847c99f5862c9fb81387956d99ec7aa" ... "e2983b237dccf3ab4937c97fa717319a9ca1a96d", ... "6c0e6080953db56375760c0471a8c5f2929baf11", ... ) >>> print("Will free " + delete_strategy.expected_freed_size_str) Will free 8.6G >>> delete_strategy.execute() Cache deletion done. Saved 8.6G. â†Collections Model Cardsâ†’ Manage huggingface_hub cache-system Understand caching Refs Blobs Snapshots .no_exist (advanced) In practice Limitations Caching assets Assets in practice Scan your cache Scan cache from the terminal Grep exampleScan cache from Python Clean your cache Delete strategy Clean cache from the terminal Using the TUIWithout TUIClean cache from Python
Hugging Face Models Datasets Spaces Docs Solutions Pricing Log In Sign Up Hub Python Library documentation Create and share Model Cards Hub Python Library Search documentation mainv0.19.3v0.18.0.rc0v0.17.3v0.16.3v0.15.1v0.14.1v0.13.4v0.12.1v0.11.0v0.10.1v0.9.1v0.8.1v0.7.0.rc0v0.6.0.rc0v0.5.1 DEENHIKO Get started Home Quickstart Installation How-to guides Overview Download files Upload files Use the CLI HfFileSystem Repository Search Inference Inference Endpoints Community Tab Collections Cache Model Cards Manage your Space Integrate a library Webhooks server Conceptual guides Git vs HTTP paradigm Reference Overview Login and logout Environment variables Managing local and online repositories Hugging Face Hub API Downloading files Mixins & serialization methods Inference Client Inference Endpoints HfFileSystem Utilities Discussions and Pull Requests Cache-system reference Repo Cards and Repo Card Data Space runtime Collections TensorBoard logger Webhooks server Join the Hugging Face community and get access to the augmented documentation experience Collaborate on models, datasets and Spaces Faster examples with accelerated inference Switch between documentation themes Sign Up to get started Create and share Model Cards The huggingface_hub library provides a Python interface to create, share, and update Model Cards. Visit the dedicated documentation page for a deeper view of what Model Cards on the Hub are, and how they work under the hood. New (beta)! Try our experimental Model Card Creator App Load a Model Card from the Hub To load an existing card from the Hub, you can use the ModelCard.load() function. Here, weâ€™ll load the card from nateraw/vit-base-beans. Copied from huggingface_hub import ModelCard card = ModelCard.load('nateraw/vit-base-beans') This card has some helpful attributes that you may want to access/leverage: card.data: Returns a ModelCardData instance with the model cardâ€™s metadata. Call .to_dict() on this instance to get the representation as a dictionary. card.text: Returns the text of the card, excluding the metadata header. card.content: Returns the text content of the card, including the metadata header. Create Model Cards From Text To initialize a Model Card from text, just pass the text content of the card to the ModelCard on init. Copied content = """ --- language: en license: mit --- # My Model Card """ card = ModelCard(content) card.data.to_dict() == {'language': 'en', 'license': 'mit'} # True Another way you might want to do this is with f-strings. In the following example, we: Use ModelCardData.to_yaml() to convert metadata we defined to YAML so we can use it to insert the YAML block in the model card. Show how you might use a template variable via Python f-strings. Copied card_data = ModelCardData(language='en', license='mit', library='timm') example_template_var = 'nateraw' content = f""" --- { card_data.to_yaml() } --- # My Model Card This model created by [@{example_template_var}](https://github.com/{example_template_var}) """ card = ModelCard(content) print(card) The above example would leave us with a card that looks like this: Copied --- language: en license: mit library: timm --- # My Model Card This model created by [@nateraw](https://github.com/nateraw) From a Jinja Template If you have Jinja2 installed, you can create Model Cards from a jinja template file. Letâ€™s see a basic example: Copied from pathlib import Path from huggingface_hub import ModelCard, ModelCardData # Define your jinja template template_text = """ --- {{ card_data }} --- # Model Card for MyCoolModel This model does this and that. This model was created by [@{{ author }}](https://hf.co/{{author}}). """.strip() # Write the template to a file Path('custom_template.md').write_text(template_text) # Define card metadata card_data = ModelCardData(language='en', license='mit', library_name='keras') # Create card from template, passing it any jinja template variables you want. # In our case, we'll pass author card = ModelCard.from_template(card_data, template_path='custom_template.md', author='nateraw') card.save('my_model_card_1.md') print(card) The resulting cardâ€™s markdown looks like this: Copied --- language: en license: mit library_name: keras --- # Model Card for MyCoolModel This model does this and that. This model was created by [@nateraw](https://hf.co/nateraw). If you update any card.data, itâ€™ll reflect in the card itself. Copied card.data.library_name = 'timm' card.data.language = 'fr' card.data.license = 'apache-2.0' print(card) Now, as you can see, the metadata header has been updated: Copied --- language: fr license: apache-2.0 library_name: timm --- # Model Card for MyCoolModel This model does this and that. This model was created by [@nateraw](https://hf.co/nateraw). As you update the card data, you can validate the card is still valid against the Hub by calling ModelCard.validate(). This ensures that the card passes any validation rules set up on the Hugging Face Hub. From the Default Template Instead of using your own template, you can also use the default template, which is a fully featured model card with tons of sections you may want to fill out. Under the hood, it uses Jinja2 to fill out a template file. Note that you will have to have Jinja2 installed to use from_template. You can do so with pip install Jinja2. Copied card_data = ModelCardData(language='en', license='mit', library_name='keras') card = ModelCard.from_template( card_data, model_id='my-cool-model', model_description="this model does this and that", developers="Nate Raw", repo="https://github.com/huggingface/huggingface_hub", ) card.save('my_model_card_2.md') print(card) Share Model Cards If youâ€™re authenticated with the Hugging Face Hub (either by using huggingface-cli login or login()), you can push cards to the Hub by simply calling ModelCard.push_to_hub(). Letâ€™s take a look at how to do thatâ€¦ First, weâ€™ll create a new repo called â€˜hf-hub-modelcards-pr-testâ€™ under the authenticated userâ€™s namespace: Copied from huggingface_hub import whoami, create_repo user = whoami()['name'] repo_id = f'{user}/hf-hub-modelcards-pr-test' url = create_repo(repo_id, exist_ok=True) Then, weâ€™ll create a card from the default template (same as the one defined in the section above): Copied card_data = ModelCardData(language='en', license='mit', library_name='keras') card = ModelCard.from_template( card_data, model_id='my-cool-model', model_description="this model does this and that", developers="Nate Raw", repo="https://github.com/huggingface/huggingface_hub", ) Finally, weâ€™ll push that up to the hub Copied card.push_to_hub(repo_id) You can check out the resulting card here. If you instead wanted to push a card as a pull request, you can just say create_pr=True when calling push_to_hub: Copied card.push_to_hub(repo_id, create_pr=True) A resulting PR created from this command can be seen here. Update metadata In this section we will see what metadata are in repo cards and how to update them. metadata refers to a hash map (or key value) context that provides some high-level information about a model, dataset or Space. That information can include details such as the modelâ€™s pipeline type, model_id or model_description. For more detail you can take a look to these guides: Model Card, Dataset Card and Spaces Settings. Now lets see some examples on how to update those metadata. Letâ€™s start with a first example: Copied >>> from huggingface_hub import metadata_update >>> metadata_update("username/my-cool-model", {"pipeline_tag": "image-classification"}) With these two lines of code you will update the metadata to set a new pipeline_tag. By default, you cannot update a key that is already existing on the card. If you want to do so, you must pass overwrite=True explicitly: Copied >>> from huggingface_hub import metadata_update >>> metadata_update("username/my-cool-model", {"pipeline_tag": "text-generation"}, overwrite=True) It often happen that you want to suggest some changes to a repository on which you donâ€™t have write permission. You can do that by creating a PR on that repo which will allow the owners to review and merge your suggestions. Copied >>> from huggingface_hub import metadata_update >>> metadata_update("someone/model", {"pipeline_tag": "text-classification"}, create_pr=True) Include Evaluation Results To include evaluation results in the metadata model-index, you can pass an EvalResult or a list of EvalResult with your associated evaluation results. Under the hood itâ€™ll create the model-index when you call card.data.to_dict(). For more information on how this works, you can check out this section of the Hub docs. Note that using this function requires you to include the model_name attribute in ModelCardData. Copied card_data = ModelCardData( language='en', license='mit', model_name='my-cool-model', eval_results = EvalResult( task_type='image-classification', dataset_type='beans', dataset_name='Beans', metric_type='accuracy', metric_value=0.7 ) ) card = ModelCard.from_template(card_data) print(card.data) The resulting card.data should look like this: Copied language: en license: mit model-index: - name: my-cool-model results: - task: type: image-classification dataset: name: Beans type: beans metrics: - type: accuracy value: 0.7 If you have more than one evaluation result youâ€™d like to share, just pass a list of EvalResult: Copied card_data = ModelCardData( language='en', license='mit', model_name='my-cool-model', eval_results = [ EvalResult( task_type='image-classification', dataset_type='beans', dataset_name='Beans', metric_type='accuracy', metric_value=0.7 ), EvalResult( task_type='image-classification', dataset_type='beans', dataset_name='Beans', metric_type='f1', metric_value=0.65 ) ] ) card = ModelCard.from_template(card_data) card.data Which should leave you with the following card.data: Copied language: en license: mit model-index: - name: my-cool-model results: - task: type: image-classification dataset: name: Beans type: beans metrics: - type: accuracy value: 0.7 - type: f1 value: 0.65 â†Cache Manage your Spaceâ†’ Create and share Model Cards Load a Model Card from the Hub Create Model Cards From Text From a Jinja Template From the Default Template Share Model Cards Update metadata Include Evaluation Results
Hugging Face Models Datasets Spaces Docs Solutions Pricing Log In Sign Up Hub Python Library documentation Manage your Space Hub Python Library Search documentation mainv0.19.3v0.18.0.rc0v0.17.3v0.16.3v0.15.1v0.14.1v0.13.4v0.12.1v0.11.0v0.10.1v0.9.1v0.8.1v0.7.0.rc0v0.6.0.rc0v0.5.1 DEENHIKO Get started Home Quickstart Installation How-to guides Overview Download files Upload files Use the CLI HfFileSystem Repository Search Inference Inference Endpoints Community Tab Collections Cache Model Cards Manage your Space Integrate a library Webhooks server Conceptual guides Git vs HTTP paradigm Reference Overview Login and logout Environment variables Managing local and online repositories Hugging Face Hub API Downloading files Mixins & serialization methods Inference Client Inference Endpoints HfFileSystem Utilities Discussions and Pull Requests Cache-system reference Repo Cards and Repo Card Data Space runtime Collections TensorBoard logger Webhooks server Join the Hugging Face community and get access to the augmented documentation experience Collaborate on models, datasets and Spaces Faster examples with accelerated inference Switch between documentation themes Sign Up to get started Manage your Space In this guide, we will see how to manage your Space runtime (secrets, hardware, and storage) using huggingface_hub. A simple example: configure secrets and hardware. Here is an end-to-end example to create and setup a Space on the Hub. 1. Create a Space on the Hub. Copied >>> from huggingface_hub import HfApi >>> repo_id = "Wauplin/my-cool-training-space" >>> api = HfApi() # For example with a Gradio SDK >>> api.create_repo(repo_id=repo_id, repo_type="space", space_sdk="gradio") 1. (bis) Duplicate a Space. This can prove useful if you want to build up from an existing Space instead of starting from scratch. It is also useful is you want control over the configuration/settings of a public Space. See duplicate_space() for more details. Copied >>> api.duplicate_space("multimodalart/dreambooth-training") 2. Upload your code using your preferred solution. Here is an example to upload the local folder src/ from your machine to your Space: Copied >>> api.upload_folder(repo_id=repo_id, repo_type="space", folder_path="src/") At this step, your app should already be running on the Hub for free ! However, you might want to configure it further with secrets and upgraded hardware. 3. Configure secrets and variables Your Space might require some secret keys, token or variables to work. See docs for more details. For example, an HF token to upload an image dataset to the Hub once generated from your Space. Copied >>> api.add_space_secret(repo_id=repo_id, key="HF_TOKEN", value="hf_api_***") >>> api.add_space_variable(repo_id=repo_id, key="MODEL_REPO_ID", value="user/repo") Secrets and variables can be deleted as well: Copied >>> api.delete_space_secret(repo_id=repo_id, key="HF_TOKEN") >>> api.delete_space_variable(repo_id=repo_id, key="MODEL_REPO_ID") From within your Space, secrets are available as environment variables (or Streamlit Secrets Management if using Streamlit). No need to fetch them via the API! Any change in your Space configuration (secrets or hardware) will trigger a restart of your app. Bonus: set secrets and variables when creating or duplicating the Space! Secrets and variables can be set when creating or duplicating a space: Copied >>> api.create_repo( ... repo_id=repo_id, ... repo_type="space", ... space_sdk="gradio", ... space_secrets=[{"key"="HF_TOKEN", "value"="hf_api_***"}, ...], ... space_variables=[{"key"="MODEL_REPO_ID", "value"="user/repo"}, ...], ... ) Copied >>> api.duplicate_space( ... from_id=repo_id, ... secrets=[{"key"="HF_TOKEN", "value"="hf_api_***"}, ...], ... variables=[{"key"="MODEL_REPO_ID", "value"="user/repo"}, ...], ... ) 4. Configure the hardware By default, your Space will run on a CPU environment for free. You can upgrade the hardware to run it on GPUs. A payment card or a community grant is required to access upgrade your Space. See docs for more details. Copied # Use `SpaceHardware` enum >>> from huggingface_hub import SpaceHardware >>> api.request_space_hardware(repo_id=repo_id, hardware=SpaceHardware.T4_MEDIUM) # Or simply pass a string value >>> api.request_space_hardware(repo_id=repo_id, hardware="t4-medium") Hardware updates are not done immediately as your Space has to be reloaded on our servers. At any time, you can check on which hardware your Space is running to see if your request has been met. Copied >>> runtime = api.get_space_runtime(repo_id=repo_id) >>> runtime.stage "RUNNING_BUILDING" >>> runtime.hardware "cpu-basic" >>> runtime.requested_hardware "t4-medium" You now have a Space fully configured. Make sure to downgrade your Space back to â€œcpu-classicâ€ when you are done using it. Bonus: request hardware when creating or duplicating the Space! Upgraded hardware will be automatically assigned to your Space once itâ€™s built. Copied >>> api.create_repo( ... repo_id=repo_id, ... repo_type="space", ... space_sdk="gradio" ... space_hardware="cpu-upgrade", ... space_storage="small", ... space_sleep_time="7200", # 2 hours in secs ... ) Copied >>> api.duplicate_space( ... from_id=repo_id, ... hardware="cpu-upgrade", ... storage="small", ... sleep_time="7200", # 2 hours in secs ... ) 5. Pause and restart your Space By default if your Space is running on an upgraded hardware, it will never be stopped. However to avoid getting billed, you might want to pause it when you are not using it. This is possible using pause_space(). A paused Space will be inactive until the owner of the Space restarts it, either with the UI or via API using restart_space(). For more details about paused mode, please refer to this section Copied # Pause your Space to avoid getting billed >>> api.pause_space(repo_id=repo_id) # (...) # Restart it when you need it >>> api.restart_space(repo_id=repo_id) Another possibility is to set a timeout for your Space. If your Space is inactive for more than the timeout duration, it will go to sleep. Any visitor landing on your Space will start it back up. You can set a timeout using set_space_sleep_time(). For more details about sleeping mode, please refer to this section. Copied # Put your Space to sleep after 1h of inactivity >>> api.set_space_sleep_time(repo_id=repo_id, sleep_time=3600) Note: if you are using a â€˜cpu-basicâ€™ hardware, you cannot configure a custom sleep time. Your Space will automatically be paused after 48h of inactivity. Bonus: set a sleep time while requesting hardware Upgraded hardware will be automatically assigned to your Space once itâ€™s built. Copied >>> api.request_space_hardware(repo_id=repo_id, hardware=SpaceHardware.T4_MEDIUM, sleep_time=3600) Bonus: set a sleep time when creating or duplicating the Space! Copied >>> api.create_repo( ... repo_id=repo_id, ... repo_type="space", ... space_sdk="gradio" ... space_hardware="t4-medium", ... space_sleep_time="3600", ... ) Copied >>> api.duplicate_space( ... from_id=repo_id, ... hardware="t4-medium", ... sleep_time="3600", ... ) 6. Add persistent storage to your Space You can choose the storage tier of your choice to access disk space that persists across restarts of your Space. This means you can read and write from disk like you would with a traditional hard drive. See docs for more details. Copied >>> from huggingface_hub import SpaceStorage >>> api.request_space_storage(repo_id=repo_id, storage=SpaceStorage.LARGE) You can also delete your storage, losing all the data permanently. Copied >>> api.delete_space_storage(repo_id=repo_id) Note: You cannot decrease the storage tier of your space once itâ€™s been granted. To do so, you must delete the storage first then request the new desired tier. Bonus: request storage when creating or duplicating the Space! Copied >>> api.create_repo( ... repo_id=repo_id, ... repo_type="space", ... space_sdk="gradio" ... space_storage="large", ... ) Copied >>> api.duplicate_space( ... from_id=repo_id, ... storage="large", ... ) More advanced: temporarily upgrade your Space ! Spaces allow for a lot of different use cases. Sometimes, you might want to temporarily run a Space on a specific hardware, do something and then shut it down. In this section, we will explore how to benefit from Spaces to finetune a model on demand. This is only one way of solving this particular problem. It has to be taken as a suggestion and adapted to your use case. Letâ€™s assume we have a Space to finetune a model. It is a Gradio app that takes as input a model id and a dataset id. The workflow is as follows: (Prompt the user for a model and a dataset) Load the model from the Hub. Load the dataset from the Hub. Finetune the model on the dataset. Upload the new model to the Hub. Step 3. requires a custom hardware but you donâ€™t want your Space to be running all the time on a paid GPU. A solution is to dynamically request hardware for the training and shut it down afterwards. Since requesting hardware restarts your Space, your app must somehow â€œrememberâ€ the current task it is performing. There are multiple ways of doing this. In this guide we will see one solution using a Dataset as â€œtask schedulerâ€. App skeleton Here is what your app would look like. On startup, check if a task is scheduled and if yes, run it on the correct hardware. Once done, set back hardware to the free-plan CPU and prompt the user for a new task. Such a workflow does not support concurrent access as normal demos. In particular, the interface will be disabled when training occurs. It is preferable to set your repo as private to ensure you are the only user. Copied # Space will need your token to request hardware: set it as a Secret ! HF_TOKEN = os.environ.get("HF_TOKEN") # Space own repo_id TRAINING_SPACE_ID = "Wauplin/dreambooth-training" from huggingface_hub import HfApi, SpaceHardware api = HfApi(token=HF_TOKEN) # On Space startup, check if a task is scheduled. If yes, finetune the model. If not, # display an interface to request a new task. task = get_task() if task is None: # Start Gradio app def gradio_fn(task): # On user request, add task and request hardware add_task(task) api.request_space_hardware(repo_id=TRAINING_SPACE_ID, hardware=SpaceHardware.T4_MEDIUM) gr.Interface(fn=gradio_fn, ...).launch() else: runtime = api.get_space_runtime(repo_id=TRAINING_SPACE_ID) # Check if Space is loaded with a GPU. if runtime.hardware == SpaceHardware.T4_MEDIUM: # If yes, finetune base model on dataset ! train_and_upload(task) # Then, mark the task as "DONE" mark_as_done(task) # DO NOT FORGET: set back CPU hardware api.request_space_hardware(repo_id=TRAINING_SPACE_ID, hardware=SpaceHardware.CPU_BASIC) else: api.request_space_hardware(repo_id=TRAINING_SPACE_ID, hardware=SpaceHardware.T4_MEDIUM) Task scheduler Scheduling tasks can be done in many ways. Here is an example how it could be done using a simple CSV stored as a Dataset. Copied # Dataset ID in which a `tasks.csv` file contains the tasks to perform. # Here is a basic example for `tasks.csv` containing inputs (base model and dataset) # and status (PENDING or DONE). # multimodalart/sd-fine-tunable,Wauplin/concept-1,DONE # multimodalart/sd-fine-tunable,Wauplin/concept-2,PENDING TASK_DATASET_ID = "Wauplin/dreambooth-task-scheduler" def _get_csv_file(): return hf_hub_download(repo_id=TASK_DATASET_ID, filename="tasks.csv", repo_type="dataset", token=HF_TOKEN) def get_task(): with open(_get_csv_file()) as csv_file: csv_reader = csv.reader(csv_file, delimiter=',') for row in csv_reader: if row[2] == "PENDING": return row[0], row[1] # model_id, dataset_id def add_task(task): model_id, dataset_id = task with open(_get_csv_file()) as csv_file: with open(csv_file, "r") as f: tasks = f.read() api.upload_file( repo_id=repo_id, repo_type=repo_type, path_in_repo="tasks.csv", # Quick and dirty way to add a task path_or_fileobj=(tasks + f"\n{model_id},{dataset_id},PENDING").encode() ) def mark_as_done(task): model_id, dataset_id = task with open(_get_csv_file()) as csv_file: with open(csv_file, "r") as f: tasks = f.read() api.upload_file( repo_id=repo_id, repo_type=repo_type, path_in_repo="tasks.csv", # Quick and dirty way to set the task as DONE path_or_fileobj=tasks.replace( f"{model_id},{dataset_id},PENDING", f"{model_id},{dataset_id},DONE" ).encode() ) â†Model Cards Integrate a libraryâ†’ Manage your Space A simple example: configure secrets and hardware. More advanced: temporarily upgrade your Space ! App skeleton Task scheduler
Hugging Face Models Datasets Spaces Docs Solutions Pricing Log In Sign Up Hub Python Library documentation Integrate any ML framework with the Hub Hub Python Library Search documentation mainv0.19.3v0.18.0.rc0v0.17.3v0.16.3v0.15.1v0.14.1v0.13.4v0.12.1v0.11.0v0.10.1v0.9.1v0.8.1v0.7.0.rc0v0.6.0.rc0v0.5.1 DEENHIKO Get started Home Quickstart Installation How-to guides Overview Download files Upload files Use the CLI HfFileSystem Repository Search Inference Inference Endpoints Community Tab Collections Cache Model Cards Manage your Space Integrate a library Webhooks server Conceptual guides Git vs HTTP paradigm Reference Overview Login and logout Environment variables Managing local and online repositories Hugging Face Hub API Downloading files Mixins & serialization methods Inference Client Inference Endpoints HfFileSystem Utilities Discussions and Pull Requests Cache-system reference Repo Cards and Repo Card Data Space runtime Collections TensorBoard logger Webhooks server Join the Hugging Face community and get access to the augmented documentation experience Collaborate on models, datasets and Spaces Faster examples with accelerated inference Switch between documentation themes Sign Up to get started Integrate any ML framework with the Hub The Hugging Face Hub makes hosting and sharing models with the community easy. It supports dozens of libraries in the Open Source ecosystem. We are always working on expanding this support to push collaborative Machine Learning forward. The huggingface_hub library plays a key role in this process, allowing any Python script to easily push and load files. There are four main ways to integrate a library with the Hub: Push to Hub: implement a method to upload a model to the Hub. This includes the model weights, as well as the model card and any other relevant information or data necessary to run the model (for example, training logs). This method is often called push_to_hub(). Download from Hub: implement a method to load a model from the Hub. The method should download the model configuration/weights and load the model. This method is often called from_pretrained or load_from_hub(). Inference API: use our servers to run inference on models supported by your library for free. Widgets: display a widget on the landing page of your models on the Hub. It allows users to quickly try a model from the browser. In this guide, we will focus on the first two topics. We will present the two main approaches you can use to integrate a library, with their advantages and drawbacks. Everything is summarized at the end of the guide to help you choose between the two. Please keep in mind that these are only guidelines that you are free to adapt to you requirements. If you are interested in Inference and Widgets, you can follow this guide. In both cases, you can reach out to us if you are integrating a library with the Hub and want to be listed in our docs. A flexible approach: helpers The first approach to integrate a library to the Hub is to actually implement the push_to_hub and from_pretrained methods by yourself. This gives you full flexibility on which files you need to upload/download and how to handle inputs specific to your framework. You can refer to the two upload files and download files guides to learn more about how to do that. This is, for example how the FastAI integration is implemented (see push_to_hub_fastai() and from_pretrained_fastai()). Implementation can differ between libraries, but the workflow is often similar. from_pretrained This is how a from_pretrained method usually look like: Copied def from_pretrained(model_id: str) -> MyModelClass: # Download model from Hub cached_model = hf_hub_download( repo_id=repo_id, filename="model.pkl", library_name="fastai", library_version=get_fastai_version(), ) # Load model return load_model(cached_model) push_to_hub The push_to_hub method often requires a bit more complexity to handle repo creation, generate the model card and save weights. A common approach is to save all of these files in a temporary folder, upload it and then delete it. Copied def push_to_hub(model: MyModelClass, repo_name: str) -> None: api = HfApi() # Create repo if not existing yet and get the associated repo_id repo_id = api.create_repo(repo_name, exist_ok=True) # Save all files in a temporary directory and push them in a single commit with TemporaryDirectory() as tmpdir: tmpdir = Path(tmpdir) # Save weights save_model(model, tmpdir / "model.safetensors") # Generate model card card = generate_model_card(model) (tmpdir / "README.md").write_text(card) # Save logs # Save figures # Save evaluation metrics # ... # Push to hub return api.upload_folder(repo_id=repo_id, folder_path=tmpdir) This is of course only an example. If you are interested in more complex manipulations (delete remote files, upload weights on the fly, persist weights locally, etc.) please refer to the upload files guide. Limitations While being flexible, this approach has some drawbacks, especially in terms of maintenance. Hugging Face users are often used to additional features when working with huggingface_hub. For example, when loading files from the Hub, it is common to offer parameters like: token: to download from a private repo revision: to download from a specific branch cache_dir: to cache files in a specific directory force_download/resume_download/local_files_only: to reuse the cache or not api_endpoint/proxies: configure HTTP session When pushing models, similar parameters are supported: commit_message: custom commit message private: create a private repo if missing create_pr: create a PR instead of pushing to main branch: push to a branch instead of the main branch allow_patterns/ignore_patterns: filter which files to upload token api_endpoint â€¦ All of these parameters can be added to the implementations we saw above and passed to the huggingface_hub methods. However, if a parameter changes or a new feature is added, you will need to update your package. Supporting those parameters also means more documentation to maintain on your side. To see how to mitigate these limitations, letâ€™s jump to our next section class inheritance. A more complex approach: class inheritance As we saw above, there are two main methods to include in your library to integrate it with the Hub: upload files (push_to_hub) and download files (from_pretrained). You can implement those methods by yourself but it comes with caveats. To tackle this, huggingface_hub provides a tool that uses class inheritance. Letâ€™s see how it works! In a lot of cases, a library already implements its model using a Python class. The class contains the properties of the model and methods to load, run, train, and evaluate it. Our approach is to extend this class to include upload and download features using mixins. A Mixin is a class that is meant to extend an existing class with a set of specific features using multiple inheritance. huggingface_hub provides its own mixin, the ModelHubMixin. The key here is to understand its behavior and how to customize it. The ModelHubMixin class implements 3 public methods (push_to_hub, save_pretrained and from_pretrained). Those are the methods that your users will call to load/save models with your library. ModelHubMixin also defines 2 private methods (_save_pretrained and _from_pretrained). Those are the ones you must implement. So to integrate your library, you should: Make your Model class inherit from ModelHubMixin. Implement the private methods:_save_pretrained(): method taking as input a path to a directory and saving the model to it. You must write all the logic to dump your model in this method: model card, model weights, configuration files, training logs, and figures. Any relevant information for this model must be handled by this method. Model Cards are particularly important to describe your model. Check out our implementation guide for more details. _from_pretrained(): class method taking as input a model_id and returning an instantiated model. The method must download the relevant files and load them. You are done! The advantage of using ModelHubMixin is that once you take care of the serialization/loading of the files, you are ready to go. You donâ€™t need to worry about stuff like repo creation, commits, PRs, or revisions. All of this is handled by the mixin and is available to your users. The Mixin also ensures that public methods are well documented and type annotated. A concrete example: PyTorch A good example of what we saw above is PyTorchModelHubMixin, our integration for the PyTorch framework. This is a ready-to-use integration. How to use it? Here is how any user can load/save a PyTorch model from/to the Hub: Copied >>> import torch >>> import torch.nn as nn >>> from huggingface_hub import PyTorchModelHubMixin # 1. Define your Pytorch model exactly the same way you are used to >>> class MyModel(nn.Module, PyTorchModelHubMixin): # multiple inheritance ... def __init__(self): ... super().__init__() ... self.param = nn.Parameter(torch.rand(3, 4)) ... self.linear = nn.Linear(4, 5) ... def forward(self, x): ... return self.linear(x + self.param) >>> model = MyModel() # 2. (optional) Save model to local directory >>> model.save_pretrained("path/to/my-awesome-model") # 3. Push model weights to the Hub >>> model.push_to_hub("my-awesome-model") # 4. Initialize model from the Hub >>> model = MyModel.from_pretrained("username/my-awesome-model") Implementation The implementation is actually very straightforward, and the full implementation can be found here. First, inherit your class from ModelHubMixin: Copied from huggingface_hub import ModelHubMixin class PyTorchModelHubMixin(ModelHubMixin): (...) Implement the _save_pretrained method: Copied from huggingface_hub import ModelCard, ModelCardData class PyTorchModelHubMixin(ModelHubMixin): (...) def _save_pretrained(self, save_directory: Path): """Generate Model Card and save weights from a Pytorch model to a local directory.""" model_card = ModelCard.from_template( card_data=ModelCardData( license='mit', library_name="pytorch", ... ), model_summary=..., model_type=..., ... ) (save_directory / "README.md").write_text(str(model)) torch.save(obj=self.module.state_dict(), f=save_directory / "pytorch_model.bin") Implement the _from_pretrained method: Copied class PyTorchModelHubMixin(ModelHubMixin): (...) @classmethod # Must be a classmethod! def _from_pretrained( cls, *, model_id: str, revision: str, cache_dir: str, force_download: bool, proxies: Optional[Dict], resume_download: bool, local_files_only: bool, token: Union[str, bool, None], map_location: str = "cpu", # additional argument strict: bool = False, # additional argument **model_kwargs, ): """Load Pytorch pretrained weights and return the loaded model.""" if os.path.isdir(model_id): # Can either be a local directory print("Loading weights from local directory") model_file = os.path.join(model_id, "pytorch_model.bin") else: # Or a model on the Hub model_file = hf_hub_download( # Download from the hub, passing same input args repo_id=model_id, filename="pytorch_model.bin", revision=revision, cache_dir=cache_dir, force_download=force_download, proxies=proxies, resume_download=resume_download, token=token, local_files_only=local_files_only, ) # Load model and return - custom logic depending on your framework model = cls(**model_kwargs) state_dict = torch.load(model_file, map_location=torch.device(map_location)) model.load_state_dict(state_dict, strict=strict) model.eval() return model And thatâ€™s it! Your library now enables users to upload and download files to and from the Hub. Quick comparison Letâ€™s quickly sum up the two approaches we saw with their advantages and drawbacks. The table below is only indicative. Your framework might have some specificities that you need to address. This guide is only here to give guidelines and ideas on how to handle integration. In any case, feel free to contact us if you have any questions! Integration Using helpers Using ModelHubMixin User experience model = load_from_hub(...)push_to_hub(model, ...) model = MyModel.from_pretrained(...)model.push_to_hub(...) Flexibility Very flexible.You fully control the implementation. Less flexible.Your framework must have a model class. Maintenance More maintenance to add support for configuration, and new features. Might also require fixing issues reported by users. Less maintenance as most of the interactions with the Hub are implemented in huggingface_hub. Documentation / Type annotation To be written manually. Partially handled by huggingface_hub. â†Manage your Space Webhooks serverâ†’ Integrate any ML framework with the Hub A flexible approach: helpers from_pretrained push_to_hub Limitations A more complex approach: class inheritance A concrete example: PyTorch How to use it?ImplementationQuick comparison
Hugging Face Models Datasets Spaces Docs Solutions Pricing Log In Sign Up Hub Python Library documentation Webhooks Server Hub Python Library Search documentation mainv0.19.3v0.18.0.rc0v0.17.3v0.16.3v0.15.1v0.14.1v0.13.4v0.12.1v0.11.0v0.10.1v0.9.1v0.8.1v0.7.0.rc0v0.6.0.rc0v0.5.1 DEENHIKO Get started Home Quickstart Installation How-to guides Overview Download files Upload files Use the CLI HfFileSystem Repository Search Inference Inference Endpoints Community Tab Collections Cache Model Cards Manage your Space Integrate a library Webhooks server Conceptual guides Git vs HTTP paradigm Reference Overview Login and logout Environment variables Managing local and online repositories Hugging Face Hub API Downloading files Mixins & serialization methods Inference Client Inference Endpoints HfFileSystem Utilities Discussions and Pull Requests Cache-system reference Repo Cards and Repo Card Data Space runtime Collections TensorBoard logger Webhooks server Join the Hugging Face community and get access to the augmented documentation experience Collaborate on models, datasets and Spaces Faster examples with accelerated inference Switch between documentation themes Sign Up to get started Webhooks Server Webhooks are a foundation for MLOps-related features. They allow you to listen for new changes on specific repos or to all repos belonging to particular users/organizations youâ€™re interested in following. This guide will explain how to leverage huggingface_hub to create a server listening to webhooks and deploy it to a Space. It assumes you are familiar with the concept of webhooks on the Huggingface Hub. To learn more about webhooks themselves, you can read this guide first. The base class that we will use in this guide is WebhooksServer(). It is a class for easily configuring a server that can receive webhooks from the Huggingface Hub. The server is based on a Gradio app. It has a UI to display instructions for you or your users and an API to listen to webhooks. To see a running example of a webhook server, check out the Spaces CI Bot one. It is a Space that launches ephemeral environments when a PR is opened on a Space. This is an experimental feature. This means that we are still working on improving the API. Breaking changes might be introduced in the future without prior notice. Make sure to pin the version of huggingface_hub in your requirements. Create an endpoint Implementing a webhook endpoint is as simple as decorating a function. Letâ€™s see a first example to explain the main concepts: Copied # app.py from huggingface_hub import webhook_endpoint, WebhookPayload @webhook_endpoint async def trigger_training(payload: WebhookPayload) -> None: if payload.repo.type == "dataset" and payload.event.action == "update": # Trigger a training job if a dataset is updated ... Save this snippet in a file called 'app.py' and run it with 'python app.py'. You should see a message like this: Copied Webhook secret is not defined. This means your webhook endpoints will be open to everyone. To add a secret, set `WEBHOOK_SECRET` as environment variable or pass it at initialization: `app = WebhooksServer(webhook_secret='my_secret', ...)` For more details about webhook secrets, please refer to https://huggingface.co/docs/hub/webhooks#webhook-secret. Running on local URL: http://127.0.0.1:7860 Running on public URL: https://1fadb0f52d8bf825fc.gradio.live This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces Webhooks are correctly setup and ready to use: - POST https://1fadb0f52d8bf825fc.gradio.live/webhooks/trigger_training Go to https://huggingface.co/settings/webhooks to setup your webhooks. Good job! You just launched a webhook server! Letâ€™s break down what happened exactly: By decorating a function with webhook_endpoint(), a WebhooksServer() object has been created in the background. As you can see, this server is a Gradio app running on http://127.0.0.1:7860. If you open this URL in your browser, you will see a landing page with instructions about the registered webhooks. A Gradio app is a FastAPI server under the hood. A new POST route /webhooks/trigger_training has been added to it. This is the route that will listen to webhooks and run the trigger_training function when triggered. FastAPI will automatically parse the payload and pass it to the function as a WebhookPayload object. This is a pydantic object that contains all the information about the event that triggered the webhook. The Gradio app also opened a tunnel to receive requests from the internet. This is the interesting part: you can configure a Webhook on https://huggingface.co/settings/webhooks pointing to your local machine. This is useful for debugging your webhook server and quickly iterating before deploying it to a Space. Finally, the logs also tell you that your server is currently not secured by a secret. This is not problematic for local debugging but is to keep in mind for later. By default, the server is started at the end of your script. If you are running it in a notebook, you can start the server manually by calling decorated_function.run(). Since a unique server is used, you only have to start the server once even if you have multiple endpoints. Configure a Webhook Now that you have a webhook server running, you want to configure a Webhook to start receiving messages. Go to https://huggingface.co/settings/webhooks, click on â€œAdd a new webhookâ€ and configure your Webhook. Set the target repositories you want to watch and the Webhook URL, here https://1fadb0f52d8bf825fc.gradio.live/webhooks/trigger_training. And thatâ€™s it! You can now trigger that webhook by updating the target repository (e.g. push a commit). Check the Activity tab of your Webhook to see the events that have been triggered. Now that you have a working setup, you can test it and quickly iterate. If you modify your code and restart the server, your public URL might change. Make sure to update the webhook configuration on the Hub if needed. Deploy to a Space Now that you have a working webhook server, the goal is to deploy it to a Space. Go to https://huggingface.co/new-space to create a Space. Give it a name, select the Gradio SDK and click on â€œCreate Spaceâ€. Upload your code to the Space in a file called app.py. Your Space will start automatically! For more details about Spaces, please refer to this guide. Your webhook server is now running on a public Space. If most cases, you will want to secure it with a secret. Go to your Space settings > Section â€œRepository secretsâ€ > â€œAdd a secretâ€. Set the WEBHOOK_SECRET environment variable to the value of your choice. Go back to the Webhooks settings and set the secret in the webhook configuration. Now, only requests with the correct secret will be accepted by your server. And this is it! Your Space is now ready to receive webhooks from the Hub. Please keep in mind that if you run the Space on a free â€˜cpu-basicâ€™ hardware, it will be shut down after 48 hours of inactivity. If you need a permanent Space, you should consider setting to an upgraded hardware. Advanced usage The guide above explained the quickest way to setup a WebhooksServer(). In this section, we will see how to customize it further. Multiple endpoints You can register multiple endpoints on the same server. For example, you might want to have one endpoint to trigger a training job and another one to trigger a model evaluation. You can do this by adding multiple @webhook_endpoint decorators: Copied # app.py from huggingface_hub import webhook_endpoint, WebhookPayload @webhook_endpoint async def trigger_training(payload: WebhookPayload) -> None: if payload.repo.type == "dataset" and payload.event.action == "update": # Trigger a training job if a dataset is updated ... @webhook_endpoint async def trigger_evaluation(payload: WebhookPayload) -> None: if payload.repo.type == "model" and payload.event.action == "update": # Trigger an evaluation job if a model is updated ... Which will create two endpoints: Copied (...) Webhooks are correctly setup and ready to use: - POST https://1fadb0f52d8bf825fc.gradio.live/webhooks/trigger_training - POST https://1fadb0f52d8bf825fc.gradio.live/webhooks/trigger_evaluation Custom server To get more flexibility, you can also create a WebhooksServer() object directly. This is useful if you want to customize the landing page of your server. You can do this by passing a Gradio UI that will overwrite the default one. For example, you can add instructions for your users or add a form to manually trigger the webhooks. When creating a WebhooksServer(), you can register new webhooks using the add_webhook() decorator. Here is a complete example: Copied import gradio as gr from fastapi import Request from huggingface_hub import WebhooksServer, WebhookPayload # 1. Define UI with gr.Blocks() as ui: ... # 2. Create WebhooksServer with custom UI and secret app = WebhooksServer(ui=ui, webhook_secret="my_secret_key") # 3. Register webhook with explicit name @app.add_webhook("/say_hello") async def hello(payload: WebhookPayload): return {"message": "hello"} # 4. Register webhook with implicit name @app.add_webhook async def goodbye(payload: WebhookPayload): return {"message": "goodbye"} # 5. Start server (optional) app.run() We define a custom UI using Gradio blocks. This UI will be displayed on the landing page of the server. We create a WebhooksServer() object with a custom UI and a secret. The secret is optional and can be set with the WEBHOOK_SECRET environment variable. We register a webhook with an explicit name. This will create an endpoint at /webhooks/say_hello. We register a webhook with an implicit name. This will create an endpoint at /webhooks/goodbye. We start the server. This is optional as your server will automatically be started at the end of the script. â†Integrate a library Git vs HTTP paradigmâ†’ Webhooks Server Create an endpoint Configure a Webhook Deploy to a Space Advanced usage Multiple endpoints Custom server
